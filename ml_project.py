# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k4Vg5l9wrUMnZPppqftiAR38sdnhGIep
"""

# upgrading the xgboost
!pip install xgboost --upgrade

# checking the version of sklearn
import sklearn
print(sklearn.__version__)

# uninstall sklearn
!pip uninstall scikit-learn

# instaall scikit 1.5.0
!pip install scikit-learn==1.5.0

"""# **0. Importing All Necessary Libraries**"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
from matplotlib.colors import ListedColormap
import warnings
warnings.filterwarnings('ignore')
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from scipy import stats
from sklearn.metrics import silhouette_score
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.preprocessing import PowerTransformer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
)
from xgboost import XGBClassifier, plot_importance

# Plot styles to set aesthetic preferences for visualizations.
sns.set()
plt.style.use('ggplot')

"""# **1. Functions Used Throughout The Project**

## **1.1 load_csv**
"""

def load_csv(file_path):
    """
    Loads a CSV file into a pandas DataFrame.

    Args:
        file_path (str): The path to the CSV file.

    Returns:
        pd.DataFrame: Loaded data as a DataFrame.
    """
    try:
        data = pd.read_csv(file_path)
        print(f"Data loaded successfully from {file_path}")
        return data
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None
    except pd.errors.EmptyDataError:
        print(f"Error: File at {file_path} is empty.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while loading {file_path}: {e}")
        return None

"""## **1.2 inspect_data**"""

def inspect_data(data, num_rows=5):
    """
    Prints basic information and a preview of the data.

    Args:
        data (pd.DataFrame): The DataFrame to inspect.
        num_rows (int): Number of rows to display in the preview.
    """
    if data is not None:
        print("\nData Preview:")
        print(data.head(num_rows))
        print("\nData Information:")
        print(data.info())
        print("\nData Summary Statistics:")
        print(data.describe())
    else:
        print("No data to inspect.")

"""## **1.3 label_dataset**"""

def label_dataset(data, label):
    """
    Adds a label column to a dataset.

    Args:
        data (pd.DataFrame): The dataset to be labeled.
        label (int): The label to assign (1 for rice, 0 for cotton).

    Returns:
        pd.DataFrame: The labeled dataset.
    """
    if not isinstance(data, pd.DataFrame):
        raise ValueError("Input data must be a pandas DataFrame.")

    # Add a 'Label' column to the dataset
    data['Class'] = label
    return data

"""## **1.4 Univariate Analysis Function For Numerical Features**"""

def analyze_numerical_feature(df, feature):
    # Statistical summary and skewness
    stats = df[feature].describe()
    skewness = df[feature].skew()
    missing_values = df[feature].isnull().sum()  # Calculate missing values

    # Set up a canvas for plotting
    fig, axs = plt.subplots(2, 2, figsize=(14, 10))

    # Histogram
    axs[0, 0].hist(df[feature].dropna(), bins=20, color='skyblue', edgecolor='black')  # Exclude missing values
    axs[0, 0].set_title(f'Histogram of {feature}')
    axs[0, 0].set_xlabel(feature)
    axs[0, 0].set_ylabel('Frequency')

    # KDE Plot
    sns.kdeplot(df[feature].dropna(), ax=axs[0, 1], color='purple', fill=True)  # Exclude missing values
    axs[0, 1].set_title(f'KDE Plot of {feature}')
    axs[0, 1].set_xlabel(feature)
    axs[0, 1].set_ylabel('Density')

    # Boxplot
    sns.boxplot(x=df[feature], ax=axs[1, 0], color='lightgreen')
    axs[1, 0].set_title(f'Boxplot of {feature}')
    axs[1, 0].set_xlabel(feature)

    # Key Metrics
    axs[1, 1].axis('off')  # Turn off the axis
    textstr = (
        f"### Key Metrics ###\n\n"
        f"{stats.to_string()}\n\n"
        f"Skewness: {skewness:.2f}\n"
        f"Missing Values: {missing_values}"
    )
    axs[1, 1].text(
        0.5, 0.5, textstr, fontsize=12, ha='center', va='center',
        bbox=dict(boxstyle="round", facecolor="lightgrey")
    )

    # Adjust layout
    plt.tight_layout()
    plt.show()

"""## **1.5 Univariate Analysis Function For Categorical Features**"""

def analyze_categorical_feature(df, column):
    """
    Perform univariate analysis for a categorical variable in a presentable manner.

    Parameters:
        df (pd.DataFrame): The dataframe containing the data.
        column (str): The column name of the categorical variable to analyze.
    """
    # Value counts
    value_counts = df[column].value_counts()
    missing_values = df[column].isnull().sum()

    # Set up subplots for bar and pie charts
    fig, axs = plt.subplots(1, 3, figsize=(18, 6))

    # Bar Plot
    value_counts.plot(kind='bar', color='skyblue', ax=axs[0])
    axs[0].set_title(f'Bar Plot of {column}', fontsize=14)
    axs[0].set_xlabel(column, fontsize=12)
    axs[0].set_ylabel('Frequency', fontsize=12)

    # Pie Chart
    value_counts.plot(kind='pie', autopct='%1.1f%%', colors=['red', 'green', 'yellow', 'orange', 'skyblue', 'violet', 'pink', 'purple', 'lightgrey', 'brown', 'grey', 'lightgreen'], ax=axs[1])
    axs[1].set_title(f'Pie Chart of {column}', fontsize=14)
    axs[1].set_ylabel('')  # Remove y-label for the pie chart

    # Summary Box
    axs[2].axis('off')  # Turn off the axis
    summary_text = f"### Analysis of '{column}' ###\n\n"
    summary_text += "Value Counts:\n"
    summary_text += "\n".join([f"{index}: {value}" for index, value in value_counts.items()])
    summary_text += f"\n\nMissing Values: {missing_values}"
    axs[2].text(0.5, 0.5, summary_text, fontsize=12, ha='center', va='center',
                bbox=dict(boxstyle="round", facecolor="lightgrey"))

    # Adjust layout
    plt.tight_layout()
    plt.show()

"""## **1.6 Bivariate Analysis Of Numerical Features**

"""

def bivariate_analysis_numerical_numerical(df, feature1, feature2):
    """
    Perform bivariate analysis between two numerical features, displaying only
    the scatter plot and the correlation coefficient, with different colors for each feature.

    Parameters:
        df (pd.DataFrame): The dataframe containing the data.
        feature1 (str): The column name of the first numerical feature.
        feature2 (str): The column name of the second numerical feature.
    """
    # Calculate correlation coefficient
    correlation_coefficient = df[[feature1, feature2]].corr().iloc[0, 1]

    # Set up a canvas for plotting with a smaller figure size
    fig, axs = plt.subplots(1, 2, figsize=(10, 5))

    # Define unique colors for the features
    feature1_color = 'skyblue'
    feature2_color = 'orange'

    # Scatter Plot with Regression Line and Different Colors
    sns.regplot(x=feature1, y=feature2, data=df, ax=axs[0], scatter_kws={'alpha': 0.6, 'color': feature1_color}, line_kws={'color': feature2_color})
    axs[0].set_title(f'Scatterplot with Regression: {feature1} vs {feature2}', fontsize=14)
    axs[0].set_xlabel(feature1, fontsize=12)
    axs[0].set_ylabel(feature2, fontsize=12)

    # Display Correlation Coefficient with different color text
    axs[1].axis('off')  # Turn off the axis
    textstr = f"### Correlation Analysis ###\n\nCorrelation Coefficient: {correlation_coefficient:.2f}"
    axs[1].text(0.5, 0.5, textstr, fontsize=14, ha='center', va='center', bbox=dict(boxstyle="round", facecolor="lightgreen"))

    # Adjust layout
    plt.tight_layout()
    plt.show()

"""## **1.7 Bivariate Analysis Of Numerical Features With Class**

"""

def bivariate_analysis_numerical_categorical(df, numerical_feature, categorical_feature):
    """
    Perform bivariate analysis between a numerical feature and a categorical feature.

    Parameters:
        df (pd.DataFrame): The dataframe containing the data.
        numerical_feature (str): The column name of the numerical feature.
        categorical_feature (str): The column name of the categorical feature.
    """
    # Set up a canvas for plotting
    fig, axs = plt.subplots(3, 1, figsize=(10, 12))  # 3 rows, 1 column layout

    # Bar Plot
    sns.barplot(x=categorical_feature, y=numerical_feature, data=df, ax=axs[0], ci=None, palette='Set2')
    axs[0].set_title(f'Bar Plot: {numerical_feature} by {categorical_feature}', fontsize=14)
    axs[0].set_xlabel(categorical_feature, fontsize=12)
    axs[0].set_ylabel(numerical_feature, fontsize=12)

    # Box Plot
    sns.boxplot(x=categorical_feature, y=numerical_feature, data=df, ax=axs[1], palette='Set3')
    axs[1].set_title(f'Box Plot: {numerical_feature} by {categorical_feature}', fontsize=14)
    axs[1].set_xlabel(categorical_feature, fontsize=12)
    axs[1].set_ylabel(numerical_feature, fontsize=12)

    # KDE Plot
    for category in df[categorical_feature].dropna().unique():
        subset = df[df[categorical_feature] == category]
        sns.kdeplot(subset[numerical_feature], ax=axs[2], label=str(category), fill=True)
    axs[2].set_title(f'KDE Plot: {numerical_feature} by {categorical_feature}', fontsize=14)
    axs[2].set_xlabel(numerical_feature, fontsize=12)
    axs[2].set_ylabel('Density', fontsize=12)
    axs[2].legend(title=categorical_feature)

    # Adjust layout
    plt.tight_layout()
    plt.show()

"""## **1.8 Time Series Plots**

### **For Rice**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def seasonal_time_series_rice(ndvi_columns, title=None):
    """
    Plots a seasonal NDVI time series for Rice across 2021, 2022, and 2023 on the same graph.

    Args:
        ndvi_columns (list): List of NDVI columns (e.g., ['NDVI_1', 'NDVI_2', ..., 'NDVI_12']).
        title (str): Optional title for the plot.

    Returns:
        None. Displays the seasonal time series plot.
    """
    # # Ensure the datasets for rice are globally available
    # global rice2021, rice2022, rice2023

    # Create a time axis for two NDVI values per month over six months
    months = [f"Month {i}" for i in range(1, (len(ndvi_columns) // 2) + 1)]
    time_steps = [f"{month} - {period}" for month in months for period in ['First_NDVI', 'Second_NDVI']]

    # Initialize the plot
    plt.figure(figsize=(12, 6))

    # Process each year's data
    for year, rice_data in zip([2021, 2022, 2023], [rice2021, rice2022, rice2023]):
        # Calculate the mean NDVI values for the year
        mean_ndvi = rice_data[ndvi_columns].mean()

        # Plot the NDVI values for the current year
        sns.lineplot(x=time_steps, y=mean_ndvi.values, marker='o', label=f"Rice {year}")

    # Customize the plot
    plt.xticks(rotation=45)
    plt.xlabel("Time (Seasonal Cycle)")
    plt.ylabel("NDVI")
    plt.title(title or "Rice NDVI Seasonal Time Series (2021-2023)")
    plt.grid(True)
    plt.legend(title="Year")
    plt.tight_layout()
    plt.show()

"""### **For Cotton**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def seasonal_time_series_cotton(ndvi_columns, title=None):
    """
    Plots a seasonal NDVI time series for Cotton across 2021, 2022, and 2023 on the same graph.

    Args:
        ndvi_columns (list): List of NDVI columns (e.g., ['NDVI_1', 'NDVI_2', ..., 'NDVI_12']).
        title (str): Optional title for the plot.

    Returns:
        None. Displays the seasonal time series plot.
    """
    # # Ensure the datasets for cotton are globally available
    # global cotton2021, cotton2022, cotton2023

    # Create a time axis for two NDVI values per month over six months
    months = [f"Month {i}" for i in range(1, (len(ndvi_columns) // 2) + 1)]
    time_steps = [f"{month} - {period}" for month in months for period in ['First_NDVI', 'Second_NDVI']]

    # Initialize the plot
    plt.figure(figsize=(12, 6))

    # Process each year's data
    for year, cotton_data in zip([2021, 2022, 2023], [cotton2021, cotton2022, cotton2023]):
        # Calculate the mean NDVI values for the year
        mean_ndvi = cotton_data[ndvi_columns].mean()

        # Plot the NDVI values for the current year
        sns.lineplot(x=time_steps, y=mean_ndvi.values, marker='o', label=f"Cotton {year}")

    # Customize the plot
    plt.xticks(rotation=45)
    plt.xlabel("Time (Seasonal Cycle)")
    plt.ylabel("NDVI")
    plt.title(title or "Cotton NDVI Seasonal Time Series (2021-2023)")
    plt.grid(True)
    plt.legend(title="Year")
    plt.tight_layout()
    plt.show()

"""## **1.9 Missing Values Checking Function**"""

def check_missing_values(dataframe):
    """
    Check for missing values in a pandas DataFrame.

    Parameters:
    dataframe (pd.DataFrame): The DataFrame to check for missing values.

    Returns:
    list: A list of column names with missing values, or a message if no missing values are present.
    """
    # Identify columns with missing values
    missing_columns = dataframe.columns[dataframe.isnull().any()].tolist()

    if missing_columns:
        print("The following columns contain missing values:")
        return missing_columns
    else:
        print("No missing values found in the dataset.")
        return []

"""## **1.10 Duplicated Values Checking Function**"""

def check_duplicates(dataframe, ignore_column='Class'):
    """
    Check for duplicate rows in a pandas DataFrame while ignoring a specific column.

    Parameters:
    dataframe (pd.DataFrame): The DataFrame to check for duplicate rows.
    ignore_column (str): The column to ignore while checking for duplicates.

    Returns:
    pd.DataFrame: A DataFrame containing duplicate rows, or a message if no duplicates are found.
    """
    # Ensure the column to ignore exists
    if ignore_column in dataframe.columns:
        # Exclude the column to ignore when checking for duplicates
        duplicates = dataframe[dataframe.drop(columns=[ignore_column]).duplicated()]
    else:
        duplicates = dataframe[dataframe.duplicated()]

    if not duplicates.empty:
        print("Duplicate rows found (ignoring column '{}'):".format(ignore_column))
        return duplicates
    else:
        print("No duplicate rows found in the dataset (ignoring column '{}').".format(ignore_column))
        return pd.DataFrame()  # Return an empty DataFrame if no duplicates

"""## **1.11 Percentage Of Duplicates**"""

def calculate_duplicate_percentage(dataframe, ignore_column='Class'):
    """
    Calculate the percentage of duplicate rows in a pandas DataFrame
    while ignoring a specific column.

    Parameters:
    dataframe (pd.DataFrame): The DataFrame to check for duplicate rows.
    ignore_column (str): The column to ignore while checking for duplicates.

    Returns:
    float: Percentage of duplicate rows in the DataFrame.
    """
    # Get duplicate rows using the check_duplicates function
    duplicates = check_duplicates(dataframe, ignore_column)

    # Calculate the percentage of duplicates
    total_rows = len(dataframe)
    duplicate_count = len(duplicates)

    if total_rows == 0:
        print("The dataset is empty. Percentage of duplicates cannot be calculated.")
        return 0.0  # Avoid division by zero

    duplicate_percentage = (duplicate_count / total_rows) * 100
    print(f"Percentage of duplicate rows (ignoring column '{ignore_column}'): {duplicate_percentage:.2f}%")
    return duplicate_percentage

"""## **1.12 Outliers Detection**"""

# 1. **IQR (Interquartile Range) Method**
def detect_outliers_iqr(df):
    Q1 = df[ndvi_columns].quantile(0.25)
    Q3 = df[ndvi_columns].quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((df[ndvi_columns] < (Q1 - 1.5 * IQR)) | (df[ndvi_columns] > (Q3 + 1.5 * IQR))).any(axis=1)

    outlier_df = df[outliers]
    print(f"Outliers found (IQR): {len(outlier_df)} rows")
    print(f"Percentage of outliers: {len(outlier_df) / len(df) * 100:.2f}%")
    return outlier_df

# 2. **Z-Score Method**
def detect_outliers_zscore(df):
    z_scores = np.abs(stats.zscore(df[ndvi_columns]))
    outliers = (z_scores > 3).any(axis=1)

    outlier_df = df[outliers]
    print(f"Outliers found (Z-Score): {len(outlier_df)} rows")
    print(f"Percentage of outliers: {len(outlier_df) / len(df) * 100:.2f}%")
    return outlier_df

# 3. **DBSCAN Method**
def detect_outliers_dbscan(df):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[ndvi_columns])

    db = DBSCAN(eps=0.2, min_samples=5)
    df['outlier'] = db.fit_predict(X_scaled)

    outlier_df = df[df['outlier'] == -1]
    print(f"Outliers found (DBSCAN): {len(outlier_df)} rows")
    print(f"Percentage of outliers: {len(outlier_df) / len(df) * 100:.2f}%")
    return outlier_df

# 4. **Isolation Forest Method**
def detect_outliers_isolation_forest(df):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[ndvi_columns])

    model = IsolationForest(contamination=0.1)  # Adjust contamination rate as needed
    df['outlier'] = model.fit_predict(X_scaled)

    outlier_df = df[df['outlier'] == -1]
    print(f"Outliers found (Isolation Forest): {len(outlier_df)} rows")
    print(f"Percentage of outliers: {len(outlier_df) / len(df) * 100:.2f}%")
    return outlier_df

def detect_outliers_lof(df):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[ndvi_columns])

    model = LocalOutlierFactor(n_neighbors=20, contamination=0.1)  # Adjust contamination rate as needed
    outlier_mask = model.fit_predict(X_scaled) == -1

    outlier_df = df[outlier_mask]
    print(f"Outliers found (LOF): {len(outlier_df)} rows")
    print(f"Percentage of outliers: {len(outlier_df) / len(df) * 100:.2f}%")
    return outlier_df

"""## **1.13 Data Balancing Functions**"""

def calculate_samples_needed(df, target_column, minority_class_label, balance_ratio=1.0):
    """
    Calculate the number of samples needed for the minority class to achieve a desired balance ratio.

    Args:
        df (pd.DataFrame): The dataset.
        target_column (str): The name of the target column.
        minority_class_label (int): The label of the minority class.
        balance_ratio (float): The desired ratio of minority class to majority class.

    Returns:
        int: The number of samples needed for the minority class.
    """
    minority_class_count = df[df[target_column] == minority_class_label].shape[0]
    majority_class_count = df[df[target_column] != minority_class_label].shape[0]
    samples_needed = int(majority_class_count * balance_ratio - minority_class_count)
    print(f"The number of samples needed for the minority class: {samples_needed}")
    return samples_needed

"""**function for random oversampling**"""

def random_oversample_dataset(df, target_column, random_state=42):
    """
    Performs random oversampling to balance the classes in the dataset.

    Parameters:
    - df: DataFrame, The entire dataset with features and target column.
    - target_column: str, The name of the target column (e.g., 'Class').
    - random_state: int, Random seed for reproducibility.

    Returns:
    - df_resampled: DataFrame, The resampled dataset with balanced classes.
    """
    # Ensure no missing values in the target column
    if df[target_column].isnull().sum() > 0:
        print(f"Warning: Missing values found in target column '{target_column}'")

    # Split the dataset into features (X) and target (y)
    X = df.drop(target_column, axis=1)
    y = df[target_column]

    # Check if the target column is categorical (as expected for classification)
    if y.dtypes != 'int64' and y.dtypes != 'object':
        raise ValueError(f"The target column '{target_column}' should be a categorical type.")

    # Apply random oversampling
    ros = RandomOverSampler(random_state=random_state)
    X_resampled, y_resampled = ros.fit_resample(X, y)

    # Combine the resampled features and target into a single DataFrame
    df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),
                              pd.Series(y_resampled, name=target_column)], axis=1)

    # Return the resampled DataFrame
    return df_resampled

"""**function for random undersampling**"""

def random_undersample_dataset(df, target_column, random_state=42):
    """
    Performs random undersampling to balance the classes in the dataset.

    Parameters:
    - df: DataFrame, The entire dataset with features and target column.
    - target_column: str, The name of the target column.
    - random_state: int, Random seed for reproducibility.

    Returns:
    - df_resampled: DataFrame, The resampled dataset with balanced classes.
    """
    # Split the dataset into features (X) and target (y)
    X = df.drop(target_column, axis=1)
    y = df[target_column]

    # Apply random undersampling
    rus = RandomUnderSampler(random_state=random_state)
    X_resampled, y_resampled = rus.fit_resample(X, y)

    # Combine the resampled features and target into a single DataFrame
    df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),
                              pd.Series(y_resampled, name=target_column)], axis=1)

    return df_resampled

# Example usage:
# df_resampled = random_undersample_dataset(df, target_column='target')

"""**function for SMOTE**"""

def smote_oversample_dataset(df, target_column, random_state=42):
    """
    Performs SMOTE oversampling to balance the classes in the dataset.

    Parameters:
    - df: DataFrame, The entire dataset with features and target column.
    - target_column: str, The name of the target column.
    - random_state: int, Random seed for reproducibility.

    Returns:
    - df_resampled: DataFrame, The resampled dataset with balanced classes.
    """
    # Split the dataset into features (X) and target (y)
    X = df.drop(target_column, axis=1)
    y = df[target_column]

    # Apply SMOTE
    smote = SMOTE(random_state=random_state)
    X_resampled, y_resampled = smote.fit_resample(X, y)

    # Combine the resampled features and target into a single DataFrame
    df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),
                              pd.Series(y_resampled, name=target_column)], axis=1)

    return df_resampled

# Example usage:
# df_resampled = smote_oversample_dataset(df, target_column='target')

"""## **1.14 Feature Scaling**"""

def scaling_features(data):
    """
    Preprocesses input features by applying Power Transformation (Yeo-Johnson)
    to handle skewness and Standardization (Z-score) to scale features.

    Parameters:
    - data (numpy.ndarray or pandas.DataFrame): 2D array-like dataset where
      each row is a sample and each column is a feature.

    Returns:
    - scaled_data (numpy.ndarray): Transformed and standardized feature dataset.
    """
    # # Step 1: Handle skewness using Power Transformation
    power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)
    transformed_data = power_transformer.fit_transform(data)

    # Step 2: Apply Standardization to scale features
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(transformed_data)

    return scaled_data

"""## **1.16 Visualizations**"""

def scatter_plot(df, target_column):
    """
    Plots the data by extracting the features and target from the dataframe.

    Parameters:
    - df: DataFrame, The complete dataset with both features and target column.
    - target_column: str, The name of the target column in the dataframe.
    """
    # Extract features and target
    X = df.drop(target_column, axis=1)
    y = df[target_column]

    # Standardize the features for better visualization
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Get the feature names for the first two features
    feature_1_name = X.columns[0]
    feature_2_name = X.columns[1]

    # Create a scatter plot of the data
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=y, palette="Set2")

    # Set title and labels
    plt.title(f'Dataset Distribution (Target: {target_column})')
    plt.xlabel(feature_1_name)
    plt.ylabel(feature_2_name)
    plt.legend(title=target_column)
    plt.show()

"""## **1.17 Supervised Learning Algorithm Related Functions**

### **XGB**
"""

def xgb_grid_search(X_train, y_train, cv=3, scoring='accuracy'):
    """
    Perform Grid Search for XGBoost hyperparameter optimization.

    Parameters:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training labels.
        cv (int): Number of cross-validation folds.
        scoring (str): Scoring metric for evaluation.

    Returns:
        best_model (XGBClassifier): Best estimator found by Grid Search.
        best_params (dict): Best hyperparameters.
    # """
    # param_grid = {
    #     'learning_rate': [0.01, 0.05, 0.1, 0.2],
    #     'max_depth': [3, 5, 7, 10],
    #     'n_estimators': [50, 100, 200, 300]
    # }

    param_grid = {
        'learning_rate': [0.1, 0.2],
        'max_depth': [ 5, 7, 10],
        'n_estimators': [50, 100,200, 300]
    }

    grid_search = GridSearchCV(
        estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
        param_grid=param_grid,
        cv=cv,
        scoring=scoring,
        n_jobs=-1,
        verbose=3
    )

    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print("Best Parameters from Grid Search:")
    print(best_params)

    return best_model, best_params

def xgb_model_training(X_train, X_test, y_train, y_test, model):
    """
    Train an XGBoost model, evaluate it, and generate visualizations.

    Parameters:
        X_train (pd.DataFrame): Training features.
        X_test (pd.DataFrame): Test features.
        y_train (pd.Series): Training labels.
        y_test (pd.Series): Test labels.
        model (XGBClassifier): Pretrained XGBoost model.

    Returns:
        metrics (dict): Performance metrics.
    """
    # Train the model
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Metrics
    metrics = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='weighted'),
        "Recall": recall_score(y_test, y_pred, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred, average='weighted')
    }

    # Print classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Feature Importance
    plt.figure(figsize=(10, 8))
    plot_importance(model, importance_type='weight', title="Feature Importance (Weight)")
    plt.show()

    # Return metrics
    return metrics

"""### **Bagging**"""

from sklearn.tree import DecisionTreeClassifier # import DecisionTreeClassifier

def bagging_grid_search(X_train, y_train, cv=3, scoring='accuracy'):
    """
    Perform Grid Search for Bagging hyperparameter optimization.

    Parameters:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training labels.
        cv (int): Number of cross-validation folds.
        scoring (str): Scoring metric for evaluation.

    Returns:
        best_model (BaggingClassifier): Best estimator found by Grid Search.
        best_params (dict): Best hyperparameters.
    """
    # param_grid = {
    #     'n_estimators': [10, 50, 100, 200],
    #     'estimator__max_depth': [None, 5, 10, 20],  # Update base_estimator__ to estimator__
    #     'max_samples': [0.5, 0.7, 1.0],
    #     'max_features': [0.5, 0.7, 1.0]
    # }

    param_grid = {
        'n_estimators': [100],
        'estimator__max_depth': [None],  # Update base_estimator__ to estimator__
        'max_samples': [ 1.0],
        'max_features': [0.7, 1.0]
    }

    grid_search = GridSearchCV(
        estimator=BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42),  # Update base_estimator to estimator
        param_grid=param_grid,
        cv=cv,
        scoring=scoring,
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print("Best Parameters from Grid Search:")
    print(best_params)

    return best_model, best_params

def bagging_model_training(X_train, X_test, y_train, y_test, model):
    """
    Train a Bagging model, evaluate it, and generate visualizations.

    Parameters:
        X_train (pd.DataFrame): Training features.
        X_test (pd.DataFrame): Test features.
        y_train (pd.Series): Training labels.
        y_test (pd.Series): Test labels.
        model (BaggingClassifier): Pretrained Bagging model.

    Returns:
        metrics (dict): Performance metrics.
    """
    # Train the model
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Metrics
    metrics = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='weighted'),
        "Recall": recall_score(y_test, y_pred, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred, average='weighted')
    }

    # Print classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Feature Importance (if the base estimator is a decision tree)
    # Changed base_estimator_ to estimator_
    if hasattr(model.estimator_, "feature_importances_"):
        importances = model.estimator_.feature_importances_
        feature_importance_df = pd.DataFrame({
            'Feature': X_train.columns,
            'Importance': importances
        }).sort_values(by='Importance', ascending=False)

        print("Feature Importance:")
        print(feature_importance_df)

        # Plot Feature Importance
        plt.figure(figsize=(10, 8))
        sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
        plt.title('Feature Importance')
        plt.show()

    return metrics

"""### **Random Forest**"""

def random_forest_grid_search(X_train, y_train, cv=3, scoring='accuracy'):
    """
    Perform Grid Search for Random Forest hyperparameter optimization.

    Parameters:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training labels.
        cv (int): Number of cross-validation folds.
        scoring (str): Scoring metric for evaluation.

    Returns:
        best_model (RandomForestClassifier): Best estimator found by Grid Search.
        best_params (dict): Best hyperparameters.
    """
    # param_grid = {
    #     'n_estimators': [10, 50, 100, 200],
    #     'max_depth': [None, 10, 20, 30],
    #     'min_samples_leaf': [1, 2, 4]
    # }
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [None, 10],
        'min_samples_leaf': [1, 2]
    }

    grid_search = GridSearchCV(
        estimator=RandomForestClassifier(random_state=42),
        param_grid=param_grid,
        cv=cv,
        scoring=scoring,
        n_jobs=-1,
        verbose=3
    )

    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print("Best Parameters from Grid Search:")
    print(best_params)

    return best_model, best_params

def random_forest_model_training(X_train, X_test, y_train, y_test, model):
    """
    Train a Random Forest model, evaluate it, and generate visualizations.

    Parameters:
        X_train (pd.DataFrame): Training features.
        X_test (pd.DataFrame): Test features.
        y_train (pd.Series): Training labels.
        y_test (pd.Series): Test labels.
        model (RandomForestClassifier): Pretrained Random Forest model.

    Returns:
        metrics (dict): Performance metrics.
    """
    # Train the model
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Metrics
    metrics = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='weighted'),
        "Recall": recall_score(y_test, y_pred, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred, average='weighted')
    }

    # Print classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Feature Importance
    importances = model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    print("Feature Importance:")
    print(feature_importance_df)

    # Plot Feature Importance
    plt.figure(figsize=(10, 8))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
    plt.title('Feature Importance')
    plt.show()

    return metrics

"""### **SVM**"""

def svm_grid_search(X_train, y_train, cv=3, scoring='accuracy'):
    """
    Perform Grid Search for SVM hyperparameter optimization.

    Parameters:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training labels.
        cv (int): Number of cross-validation folds.
        scoring (str): Scoring metric for evaluation.

    Returns:
        best_model (SVC): Best estimator found by Grid Search.
        best_params (dict): Best hyperparameters.
    """
    param_grid = {
        'kernel': ['linear', 'rbf',],
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto']  # Only applicable for 'rbf' and 'poly' kernels
    }

    grid_search = GridSearchCV(
        estimator=SVC(random_state=42),
        param_grid=param_grid,
        cv=cv,
        scoring=scoring,
        n_jobs=-1,
        verbose=3
    )

    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print("Best Parameters from Grid Search:")
    print(best_params)

    return best_model, best_params

def svm_model_training(X_train, X_test, y_train, y_test, model):
    """
    Train an SVM model, evaluate it, and generate visualizations.

    Parameters:
        X_train (pd.DataFrame): Training features.
        X_test (pd.DataFrame): Test features.
        y_train (pd.Series): Training labels.
        y_test (pd.Series): Test labels.
        model (SVC): Pretrained SVM model.

    Returns:
        metrics (dict): Performance metrics.
    """
    # Train the model
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Metrics
    metrics = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='weighted'),
        "Recall": recall_score(y_test, y_pred, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred, average='weighted')
    }

    # Print classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    return metrics

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

def visualize_svm_results(X_train, y_train, X_test, y_test, model, title="SVM Visualization"):
    """
    Visualizes the SVM model performance:
    - Decision boundary (in 2D using PCA for high-dimensional data).
    - Decision regions for SVM predictions.

    Parameters:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training labels.
        X_test (pd.DataFrame): Test features.
        y_test (pd.Series): Test labels.
        model (SVC): Trained SVM model.
        title (str): Title for the visualization.
    """
    # Reduce dimensions to 2D for visualization using PCA
    pca = PCA(n_components=2)
    X_train_2D = pca.fit_transform(X_train)
    X_test_2D = pca.transform(X_test)

    # Fit the SVM model again using the reduced 2D data
    model.fit(X_train_2D, y_train)

    # Plot decision boundary
    plt.figure(figsize=(10, 8))
    x_min, x_max = X_train_2D[:, 0].min() - 1, X_train_2D[:, 0].max() + 1
    y_min, y_max = X_train_2D[:, 1].min() - 1, X_train_2D[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))

    # Predict the decision regions
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot the decision boundary and training/test points
    plt.contourf(xx, yy, Z, alpha=0.2, cmap='RdBu')
    plt.contour(xx, yy, Z, colors='k', linewidths=0.5)  # Add boundary lines for better clarity

    # Plot training points
    plt.scatter(X_train_2D[:, 0], X_train_2D[:, 1], c=y_train, cmap='bwr', edgecolors='k', s=20, label='Train Data')
    # Plot test points
    plt.scatter(X_test_2D[:, 0], X_test_2D[:, 1], c=y_test, cmap='bwr', marker='x', s=50, linewidths=1.5, label='Test Data')

    # Add labels and legend
    plt.title(title)
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.legend()
    plt.grid()
    plt.show()

"""## **1.18 Unsupervised Learning Algorithm Related Functions**

### **PCA**
"""

def apply_pca(data, n_components=2):
    """
    Apply PCA to reduce dimensions of the dataset and return a Pandas DataFrame.

    Parameters:
    data (pd.DataFrame): Input dataset.
    n_components (int): Number of principal components to keep.

    Returns:
    pd.DataFrame: Transformed dataset after PCA as a Pandas DataFrame.
    """
    if not isinstance(data, pd.DataFrame):
        raise ValueError("Input data must be a Pandas DataFrame.")

    # Perform PCA
    pca = PCA(n_components=n_components)
    reduced_data = pca.fit_transform(data)

    # Create column names for the principal components
    component_names = [f'PC{i+1}' for i in range(n_components)]

    # Return the transformed data as a Pandas DataFrame
    return pd.DataFrame(reduced_data, columns=component_names)

"""### **K-Means Clustering**

#### **Finding Optimal K**
"""

def find_optimal_k(X, max_k=10, random_state=42):
    """
    Determine the optimal number of clusters using the Elbow Method.

    Parameters:
    X (np.ndarray): Feature data for clustering.
    max_k (int): Maximum number of clusters to test (default: 10).
    random_state (int): Random state for reproducibility (default: 42).

    Returns:
    None
    """
    inertia = []
    k_values = range(1, max_k + 1)

    # Calculate inertia for each k
    for k in k_values:
        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=random_state)
        kmeans.fit(X)
        inertia.append(kmeans.inertia_)

    # Plot the Elbow Method
    plt.figure(figsize=(10, 5))
    plt.plot(k_values, inertia, marker='o', linestyle='--')
    plt.title("Elbow Method to Determine Optimal k")
    plt.xlabel("Number of Clusters (k)")
    plt.ylabel("Inertia (Sum of Squared Distances)")
    plt.xticks(k_values)
    plt.grid(True)
    plt.show()

"""#### **K means Implementation**"""

def kmeans_clustering(data, n_clusters):
    """
    Perform K-Means clustering.

    Parameters:
    data (np.ndarray): Input dataset.
    n_clusters (int): Number of clusters.

    Returns:
    np.ndarray: Cluster labels.
    """
    model = KMeans(n_clusters=n_clusters, random_state=42)
    labels = model.fit_predict(data)
    return model, labels

"""### **Hierarchical Clustering**

#### **Implementation**
"""

def hierarchical_clustering(data, n_clusters, method="ward"):
    """
    Perform hierarchical clustering.

    Parameters:
    data (np.ndarray): Input dataset.
    n_clusters (int): Number of clusters.
    method(string): Method of linkage.

    Returns:
    np.ndarray: Cluster labels.
    """
    linkage_matrix = linkage(data, method=method)
    labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')

    return labels

"""#### **Dendrogram**"""

def plot_dendrogram(data, method='ward', title='Hierarchical Clustering Dendrogram', max_depth=5):
    """
    Create and plot a dendrogram for hierarchical clustering, truncated at a specific level to avoid recursion errors.

    Parameters:
    data (np.ndarray): Input dataset for clustering.
    method (str): Linkage method to compute the hierarchical clustering ('ward', 'complete', 'average', etc.).
    title (str): Title of the dendrogram plot.
    max_depth (int): The maximum depth to show in the dendrogram.
    """
    # Compute the linkage matrix for different methods
    Z1 = linkage(data, method='single', metric='euclidean')
    Z2 = linkage(data, method='complete', metric='euclidean')
    Z3 = linkage(data, method='average', metric='euclidean')
    Z4 = linkage(data, method='ward', metric='euclidean')

    # Plotting subplots with truncated dendrograms
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 2, 1)
    dendrogram(Z1, truncate_mode='level', p=max_depth)
    plt.title(f'Single Linkage (max depth {max_depth})')

    plt.subplot(2, 2, 2)
    dendrogram(Z2, truncate_mode='level', p=max_depth)
    plt.title(f'Complete Linkage (max depth {max_depth})')

    plt.subplot(2, 2, 3)
    dendrogram(Z3, truncate_mode='level', p=max_depth)
    plt.title(f'Average Linkage (max depth {max_depth})')

    plt.subplot(2, 2, 4)
    dendrogram(Z4, truncate_mode='level', p=max_depth)
    plt.title(f'Ward Linkage (max depth {max_depth})')

    plt.tight_layout()
    plt.show()

"""### **DBScan Clustering**

#### **Finding Optimal epsilon**
"""

def find_optimal_epsilon(X, k=24):
    """
    Determine the optimal epsilon (eps) for DBSCAN using a k-distance graph.

    Parameters:
    X (np.ndarray): Feature data for clustering.
    k (int): Number of nearest neighbors to consider (default: 13).

    Returns:
    None
    """
    # Fit NearestNeighbors to find k distances
    neighbors = NearestNeighbors(n_neighbors=k)
    neighbors_fit = neighbors.fit(X)
    distances, indices = neighbors_fit.kneighbors(X)

    # Sort distances for plotting
    distances = np.sort(distances[:, -1])

    # Plot the k-distance graph
    plt.figure(figsize=(10, 5))
    plt.plot(distances)
    plt.title(f"K-Distance Graph (k={k})")
    plt.xlabel("Points sorted by distance")
    plt.ylabel("Epsilon")
    plt.grid(True)
    plt.show()

"""#### **Implementation**"""

def dbscan_clustering(data, eps, min_samples):
    """
    Perform DBSCAN clustering.

    Parameters:
    data (np.ndarray): Input dataset.
    eps (float): The maximum distance between two samples for them to be considered neighbors.
    min_samples (int): The minimum number of samples required to form a dense region.

    Returns:
    np.ndarray: Cluster labels.
    """
    model = DBSCAN(eps=eps, min_samples=min_samples)
    labels = model.fit_predict(data)
    return labels

def dbscan_experiment(X, eps_range, min_samples_range, pca_df):
    """
    Experiment with DBSCAN clustering for different combinations of parameters.

    Parameters:
    X (np.ndarray): Standardized dataset.
    eps_range (list): List of eps values to test.
    min_samples_range (list): List of min_samples values to test.
    pca_df (pd.DataFrame): DataFrame containing PCA-transformed data.
    """
    for eps in eps_range:
        for min_samples in min_samples_range:
            # Apply DBSCAN
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = dbscan.fit_predict(X)

            # Visualize clustering results
            title = f'DBSCAN (eps={eps}, min_samples={min_samples})'
            visualize_clusters(pca_df.values, cluster_labels, title)

            # Compute cluster statistics
            unique_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            noise_points = sum(cluster_labels == -1)

            # Print cluster information
            print(f"eps={eps}, min_samples={min_samples}")
            print(f"Number of clusters (excluding noise): {unique_clusters}")
            print(f"Number of noise points: {noise_points}")
            print("-" * 30)

             # Compute silhouette score only if there are at least 2 clusters (excluding noise)
            if unique_clusters > 1:
                silhouette = silhouette_score(X, cluster_labels)
                print("Silhouette score: ", silhouette)
            else:
                print("Silhouette score: Not enough clusters for valid evaluation.")

"""### **GMM Clustering**"""

def gmm_clustering(data, n_clusters):
    """
    Perform clustering using Gaussian Mixture Model (GMM).

    Parameters:
    data (np.ndarray): Input dataset.
    n_components (int): Number of mixture components (clusters).

    Returns:
    np.ndarray: Cluster labels.
    """
    model = GaussianMixture(n_components=n_clusters, random_state=42)
    labels = model.fit_predict(data)
    probabilities = model.predict_proba(data)
    print("Probabilities: ", probabilities)  # Probabilities for each point in each cluster
    log_likelihood = model.score(data)
    print("Log-Likelihood:", log_likelihood)

    return labels

"""### **Evaluate Clustering**"""

def evaluate_clustering(labels, true_labels, data=None):
    """
    Evaluate clustering performance using confusion matrix, NMI, cluster purity, and silhouette score.
    Automatically maps predicted cluster labels to true labels based on the majority class in each cluster.
    Visualize the confusion matrix as a heatmap.

    Parameters:
    labels (np.ndarray): Predicted cluster labels.
    true_labels (np.ndarray): Ground truth labels.
    data (np.ndarray, optional): Original dataset for silhouette score calculation.

    Returns:
    dict: Evaluation metrics (Confusion Matrix, NMI, Cluster Purity, Silhouette Score).
    """
    # Map cluster labels to true labels
    def map_clusters_to_true_labels(true_labels, cluster_labels):
        """
        Map cluster labels to true labels based on the majority class in each cluster.

        Parameters:
        true_labels (np.ndarray): True labels of the dataset.
        cluster_labels (np.ndarray): Predicted cluster labels.

        Returns:
        np.ndarray: Cluster labels mapped to true labels.
        """
        unique_clusters = np.unique(cluster_labels)
        mapped_labels = np.zeros_like(cluster_labels)

        for cluster in unique_clusters:
            cluster_indices = np.where(cluster_labels == cluster)[0]
            true_class = np.bincount(true_labels[cluster_indices]).argmax()
            mapped_labels[cluster_indices] = true_class

        return mapped_labels

    # Map predicted labels
    mapped_labels = map_clusters_to_true_labels(true_labels, labels)

    # Compute confusion matrix
    cm = confusion_matrix(true_labels, mapped_labels)

    # Calculate evaluation metrics
    nmi = normalized_mutual_info_score(true_labels, labels)
    purity = np.sum(np.amax(cm, axis=0)) / np.sum(cm)
    silhouette = None

    if data is not None:
        try:
            silhouette = silhouette_score(data, labels)
        except ValueError as e:
            print(f"Error calculating silhouette score: {e}")

    # Print metrics
    print('Confusion Matrix:')
    print(cm)
    print('Normalized Mutual Information (NMI):', nmi)
    print('Cluster Purity:', purity)
    if silhouette is not None:
        print('Silhouette Score:', silhouette)

    # Plot heatmap for confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
    plt.title('Confusion Matrix Heatmap')
    plt.xlabel('Predicted Clusters')
    plt.ylabel('True Labels')
    plt.show()

def plot_comparison(data, true_labels, predicted_labels):
    """
    Compare the clustering results with true labels via visualization.

    Parameters:
    data (np.ndarray or pd.DataFrame): Dataset to visualize.
    true_labels (np.ndarray): Ground truth class labels.
    predicted_labels (np.ndarray): Labels predicted by the clustering model.
    """

    # Ensure data is a NumPy array for compatibility
    if isinstance(data, pd.DataFrame):
        data = data.values

    # Plotting True Labels
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.scatter(data[:, 0], data[:, 1], c=true_labels, cmap='viridis', s=50, alpha=0.7)
    plt.title("True Labels")
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.colorbar(label='True Class')
    plt.grid(True)

    # Plotting Predicted Labels
    plt.subplot(1, 2, 2)
    plt.scatter(data[:, 0], data[:, 1], c=predicted_labels, cmap='viridis', s=50, alpha=0.7)
    plt.title("Predicted Clusters")
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.colorbar(label='Cluster')
    plt.grid(True)

    # Show the plots
    plt.tight_layout()
    plt.show()

"""### **Visualize Clustering**"""

def visualize_clusters(data, labels, title, centroids=None):
    """
    Visualize clustering results using a scatter plot, with optional centroids.

    Parameters:
    data (pd.DataFrame or np.ndarray): Dataset for visualization (2D).
    labels (np.ndarray): Cluster labels.
    title (str): Plot title.
    centroids (np.ndarray, optional): Centroid coordinates for the clusters.
    """
    # If data is a DataFrame, convert it to numpy array for uniform handling
    if isinstance(data, pd.DataFrame):
        data = data.values

    # Scatter plot of the data points
    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7, label='Data Points')

    # If centroids are provided, plot them
    if centroids is not None:
        plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', label='Centroids', marker='^')
        print("Centroids are: ")
        print(centroids)

    # Add plot title and labels
    plt.title(title)
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.colorbar(label='Cluster')
    plt.legend()
    plt.grid(True)
    plt.show()

from matplotlib import cm
import matplotlib.pyplot as plt


def map_features_to_indices(feature_names, feature_pairs):
    """
    Map feature names to their corresponding indices.

    Parameters:
    feature_names (list of str): List of feature names.
    feature_pairs (list of tuples): Feature name pairs.

    Returns:
    list of tuples: Converted index pairs.
    """
    feature_index_map = {name: idx for idx, name in enumerate(feature_names)}  # Map feature names to indices
    index_pairs = []
    for f1, f2 in feature_pairs:
        try:
            index_pairs.append((feature_index_map[f1], feature_index_map[f2]))
        except KeyError as e:
            print(f"KeyError: Feature name {e} is missing from feature_names.")
    return index_pairs


def plot_clusters_with_features(X, pred, feature_pairs, feature_names, cluster_centers=None,):
    """
    Plot clusters on specified feature pairs with centroids.

    Parameters:
    X (pd.DataFrame or numpy.ndarray): The feature dataset.
    pred (numpy.ndarray): Cluster labels predicted by the clustering algorithm.
    cluster_centers (numpy.ndarray): The coordinates of cluster centers.
    feature_pairs (list of tuple): List of feature index pairs to plot.
                                   Example: [(0, 1), (2, 3)].
    feature_names (list of str): Names of the features, to label the axes.
                                 Example: ["Feature 1", "Feature 2", "Feature 3", "Feature 4"].
    """
    # Ensure X is a NumPy array
    if isinstance(X, pd.DataFrame):
        X = X.to_numpy()

    # Check shape and type of X
    print("X shape:", X.shape)

    num_plots = len(feature_pairs)
    plt.figure(figsize=(6 * num_plots, 5))

    for i, (f1, f2) in enumerate(feature_pairs):
        # Debugging: print indices being accessed
        print(f"Using feature indices: {f1}, {f2}")

        # Safety check for index validity
        if f1 >= X.shape[1] or f2 >= X.shape[1]:
            print(f"Error: Indices {f1}, {f2} are out of bounds for data with shape {X.shape}")
            continue

        # Scatter plot for clusters
        plt.subplot(1, num_plots, i + 1)
        plt.scatter(X[:, f1], X[:, f2], c=pred, cmap=cm.Accent, s=50)
        plt.grid(True)

        if cluster_centers is not None:
          # Plot cluster centroids
          for center in cluster_centers:
              plt.scatter(center[f1], center[f2], marker='^', c='red', s=100, label='Centroids' if i == 0 else "")

        # Axis labels
        plt.xlabel(feature_names[f1])
        plt.ylabel(feature_names[f2])
        plt.title(f"Clustering on {feature_names[f1]} vs {feature_names[f2]}")

    plt.legend()
    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt
import numpy as np

def cluster_class_distribution(cluster_labels, true_labels):
    """
    Create bar charts showing the distribution of true labels in each cluster
    and print a summary of the data distribution.

    Parameters:
    - cluster_labels: numpy.ndarray, predicted cluster labels (e.g., from k-means).
    - true_labels: numpy.ndarray, the actual true class labels of the data points.
    """
    # Get the unique cluster labels and true class labels
    clusters = np.unique(cluster_labels)
    classes = np.unique(true_labels)

    # Initialize a dictionary to store counts of true labels in each cluster
    counts_matrix = np.zeros((len(clusters), len(classes)))

    # Count true label occurrences in each cluster
    for cluster_id in clusters:
        for class_id in classes:
            count = np.sum((cluster_labels == cluster_id) & (true_labels == class_id))

            # Map cluster_id and class_id to zero-based indices if necessary
            cluster_idx = np.where(clusters == cluster_id)[0][0]  # Get the index for cluster_id
            class_idx = np.where(classes == class_id)[0][0]  # Get the index for class_id

            counts_matrix[cluster_idx, class_idx] = count

    # Print the summary
    for cluster_id in clusters:
        cluster_summary = [f"{int(counts_matrix[np.where(clusters == cluster_id)[0][0], np.where(classes == class_id)[0][0]])} datapoints with label {class_id}"
                           for class_id in classes]
        print(f"Cluster {cluster_id} has " + " and ".join(cluster_summary) + ".")

    # Plotting the bar charts
    fig, ax = plt.subplots(figsize=(12, 8))

    # Create bar positions
    bar_width = 0.8 / len(classes)  # Divide space evenly for each class in each cluster
    indices = np.arange(len(clusters))

    # Plot each class as a grouped bar in each cluster
    for i, class_id in enumerate(classes):
        ax.bar(
            indices + i * bar_width,
            counts_matrix[:, i],
            bar_width,
            label=f"Class {class_id}",
        )

    # Configure the plot
    ax.set_xlabel('Cluster')
    ax.set_ylabel('Number of Instances')
    ax.set_xticks(indices + bar_width * (len(classes) - 1) / 2)  # Center the groups
    ax.set_xticklabels([f"Cluster {i}" for i in clusters])
    ax.legend()
    ax.grid(axis="y", linestyle='--', alpha=0.7)

    # Display the chart
    plt.title('Distribution of True Classes in Each Cluster')
    plt.tight_layout()
    plt.show()

"""# **2. Dataset Overview**

## **2.1 Mounting Google Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **2.2 Uploading The Dataset Files**"""

# LOADING THE DATA OF COTTON AND RICE

  # defining the path variables
  # RICE
  rice2021 = '/content/drive/MyDrive/CropClassificationProject/1_data/Rice/rice2021.csv'
  rice2022 = '/content/drive/MyDrive/CropClassificationProject/1_data/Rice/rice2022.csv'
  rice2023 = '/content/drive/MyDrive/CropClassificationProject/1_data/Rice/rice2023.csv'
  # COTTON
  cotton2021 = '/content/drive/MyDrive/CropClassificationProject/1_data/Cotton/cotton2021.csv'
  cotton2022 = '/content/drive/MyDrive/CropClassificationProject/1_data/Cotton/cotton2022.csv'
  cotton2023 = '/content/drive/MyDrive/CropClassificationProject/1_data/Cotton/cotton2023.csv'


  # CALLING THE load_csv FUNCTION FROM preprocessing/load_data.py
  # RICE
  rice2021 = load_csv(rice2021)
  rice2022 = load_csv(rice2022)
  rice2023 = load_csv(rice2023)
  # COTTON
  cotton2021 = load_csv(cotton2021)
  cotton2022 = load_csv(cotton2022)
  cotton2023 = load_csv(cotton2023)

"""## **2.3 Inspection Of Rice Dataset**"""

print('INSPECTION OF DATASET OF RICE FOR YEAR 2021')
  inspect_data(rice2021)

print('INSPECTION OF DATASET OF RICE FOR YEAR 2022')
  inspect_data(rice2022)

print('INSPECTION OF DATASET OF RICE FOR YEAR 2023')
  inspect_data(rice2023)

"""## **2.4 Inspection Of Cotton Dataset**"""

print('INSPECTION OF DATASET OF COTTON FOR YEAR 2021')
  inspect_data(cotton2021)

print('INSPECTION OF DATASET OF COTTON FOR YEAR 2022')
  inspect_data(cotton2022)

print('INSPECTION OF DATASET OF COTTON FOR YEAR 2023')
  inspect_data(cotton2023)

"""## **Summary Of Dataset Overview**


*   All the features are of float datatypes.
*   Class Labelling for rice and cotton is required.

# **3. Data Labelling And Merging**

## **3.1 Labelling The Rice And Cotton Data**
"""

# LABELLING THE DATA OF RICE
rice2021 = label_dataset(rice2021, 1)
rice2022 = label_dataset(rice2022, 1)
rice2023 = label_dataset(rice2023, 1)

# LABELLING THE DATA OF COTTON
cotton2021 = label_dataset(cotton2021, 0)
cotton2022 = label_dataset(cotton2022, 0)
cotton2023 = label_dataset(cotton2023, 0)

"""## **3.2 Validating The Labels Assignment**"""

inspect_data(rice2021)

inspect_data(rice2022)

inspect_data(rice2023)

inspect_data(cotton2021)

inspect_data(cotton2022)

inspect_data(cotton2023)

"""## **3.3 Merging The Yearwise Data**"""

# Combine datasets for Year 1
year1 = pd.concat([rice2021, cotton2021], axis=0)
year2 = pd.concat([rice2022, cotton2022], axis=0)
year3 = pd.concat([rice2023, cotton2023], axis=0)

"""## **3.4 Storing The Yearwise Data**"""

# Save the combined data
year1.to_csv('/content/drive/MyDrive/CropClassificationProject/1_data/merged_data/year1.csv', index=False)
year2.to_csv('/content/drive/MyDrive/CropClassificationProject/1_data/merged_data/year2.csv', index=False)
year3.to_csv('/content/drive/MyDrive/CropClassificationProject/1_data/merged_data/year3.csv', index=False)

# SHAPE OF DATA BEFORE MERGING

print("Before Merging:")
print("RICE2021 ", rice2021.shape)
print("RICE2022 ", rice2022.shape)
print("RICE2023 ", rice2023.shape)
print("COTTON2021 ", cotton2021.shape)
print("COTTON2022 ", cotton2022.shape)
print("COTTON2023 ", cotton2023.shape)

print("After Merging:")
print("YEAR1 ", year1.shape)
print("YEAR2 ", year2.shape)
print("YEAR3 ", year3.shape)

"""# **4. Exploratory Data Analysis**

## **4.1 Exploratory Data Analysis For Year 1**

### **4.1.1 Univariate Analysis**

Univariate analysis focuses on analyzing each feature in the dataset independently.


*   **Distribution analysis:** The distribution of each feature is examined to identify its shape, central tendency, and dispersion
*   **Identifying potential issues:** Univariate analysis helps in identifying potential problems with the data such as outliers, skewness and missing values

#### **4.1.1.1 Steps of doing Univariate Analysis on Numerical columns**

**Descriptive Statistics:** Compute basic summary statistics for the column, such as mean, median, mode, standard deviation, range, and quartiles. These statistics give a general understanding of the distribution of the data and can help identify skewness or outliers.

**Visualizations:** Create visualizations to explore the distribution of the data. Some common visualizations for numerical data include histograms, box plots, and density plots. These visualizations provide a visual representation of the distribution of the data and can help identify skewness an outliers.

**Data Imbalances**

**Identifying Outliers:** Identify and examine any outliers in the data. Outliers can be identified using visualizations. It is important to determine whether the outliers are due to measurement errors, data entry errors, or legitimate differences in the data, and to decide whether to include or exclude them from the analysis.

**Skewness:** Check for skewness in the data and consider transforming the data or using robust statistical methods that are less sensitive to skewness, if necessary.

**Conclusion:** Summarize the findings of the EDA and make decisions about how to proceed with further analysis.

#### **4.1.1.2 Numerical Variables**
"""

for col in year1.columns:
    if year1[col].dtype == 'float64':
        analyze_numerical_feature(year1, col)

"""#### **4.1.1.3 Categorical Variables**"""

analyze_categorical_feature(year1, 'Class')

"""### **4.1.2 Bivariate Analysis**

#### **4.1.2.1 Steps of doing Bivariate Analysis**

- Select 2 cols
- Understand type of relationship
    1. **Numerical - Numerical**<br>
        a. You can plot graphs like scatterplot(regression plots), 2D histplot, 2D KDEplots<br>
        b. Check correlation coefficent to check linear relationship
    2. **Numerical - Categorical** - create visualizations that compare the distribution of the numerical data across different categories of the categorical data.<br>
        a. You can plot graphs like barplot, boxplot, kdeplot violinplot even scatterplots. Scatterplots are not as much beneficial<br>
    3. **Categorical - Categorical**<br>
        a. You can create cross-tabulations or contingency tables that show the distribution of values in one categorical column, grouped by the values in the other categorical column.<br>
        b. You can plots like heatmap, stacked barplots, treemaps

#### **4.1.2.2 First Type: Numerical-Numerical**

##### **NDVI01 with all other NDVI values**
"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01':
        bivariate_analysis_numerical_numerical(year1, 'NDVI01', col)

"""##### **NDVI02 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02':
        bivariate_analysis_numerical_numerical(year1, 'NDVI02', col)

"""##### **NDVI03 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' :
        bivariate_analysis_numerical_numerical(year1, 'NDVI03', col)

"""##### **NDVI04 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04':
        bivariate_analysis_numerical_numerical(year1, 'NDVI04', col)

"""##### **NDVI05 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05':
        bivariate_analysis_numerical_numerical(year1, 'NDVI05', col)

"""##### **NDVI06 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06':
        bivariate_analysis_numerical_numerical(year1, 'NDVI06', col)

"""##### **NDVI07 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07':
        bivariate_analysis_numerical_numerical(year1, 'NDVI07', col)

"""##### **NDVI08 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08':
        bivariate_analysis_numerical_numerical(year1, 'NDVI08', col)

"""##### **NDVI09 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08' and col!= 'NDVI09':
        bivariate_analysis_numerical_numerical(year1, 'NDVI09', col)

"""##### **NDVI10 with all other NDVI values**"""

for col in year1.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08' and col != 'NDVI09' and col != 'NDVI10':
        bivariate_analysis_numerical_numerical(year1, 'NDVI10', col)

"""##### **NDVI11 with all other NDVI values**"""

bivariate_analysis_numerical_numerical(year1, 'NDVI11', 'NDVI12')

"""#### **4.1.2.3 Second Type: Numerical-Categorical**"""

# all numerical features with categorical
for col in year1.columns:
    if year1[col].dtype == 'float64':
        bivariate_analysis_numerical_categorical(year1, col, 'Class')

"""#### **4.1.2.4 Third Type: Categorical-Categoricals**

**no features for this combination avaiable for analysis**

## **4.2 Exploratory Data Analysis For Year 2**

### **4.2.1 Univariate Analysis**

Univariate analysis focuses on analyzing each feature in the dataset independently.


*   **Distribution analysis:** The distribution of each feature is examined to identify its shape, central tendency, and dispersion
*   **Identifying potential issues:** Univariate analysis helps in identifying potential problems with the data such as outliers, skewness and missing values

#### **4.2.1.1 Steps of doing Univariate Analysis on Numerical columns**

**Descriptive Statistics:** Compute basic summary statistics for the column, such as mean, median, mode, standard deviation, range, and quartiles. These statistics give a general understanding of the distribution of the data and can help identify skewness or outliers.

**Visualizations:** Create visualizations to explore the distribution of the data. Some common visualizations for numerical data include histograms, box plots, and density plots. These visualizations provide a visual representation of the distribution of the data and can help identify skewness an outliers.

**Data Imbalances**

**Identifying Outliers:** Identify and examine any outliers in the data. Outliers can be identified using visualizations. It is important to determine whether the outliers are due to measurement errors, data entry errors, or legitimate differences in the data, and to decide whether to include or exclude them from the analysis.

**Skewness:** Check for skewness in the data and consider transforming the data or using robust statistical methods that are less sensitive to skewness, if necessary.

**Conclusion:** Summarize the findings of the EDA and make decisions about how to proceed with further analysis.

#### **4.2.1.2 Numerical Variables**
"""

for col in year2.columns:
    if year2[col].dtype == 'float64':
        analyze_numerical_feature(year2, col)

"""#### **4.2.1.3 Categorical Variables**"""

analyze_categorical_feature(year2, 'Class')

"""### **4.2.2 Bivariate Analysis**

#### **4.2.2.1 Steps of doing Bivariate Analysis**

- Select 2 cols
- Understand type of relationship
    1. **Numerical - Numerical**<br>
        a. You can plot graphs like scatterplot(regression plots), 2D histplot, 2D KDEplots<br>
        b. Check correlation coefficent to check linear relationship
    2. **Numerical - Categorical** - create visualizations that compare the distribution of the numerical data across different categories of the categorical data.<br>
        a. You can plot graphs like barplot, boxplot, kdeplot violinplot even scatterplots. Scatterplots are not as much beneficial<br>
    3. **Categorical - Categorical**<br>
        a. You can create cross-tabulations or contingency tables that show the distribution of values in one categorical column, grouped by the values in the other categorical column.<br>
        b. You can plots like heatmap, stacked barplots, treemaps

#### **4.2.2.2 First Type: Numerical-Numerical**

##### **NDVI01 with all other NDVI values**
"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01':
        bivariate_analysis_numerical_numerical(year2, 'NDVI01', col)

"""##### **NDVI02 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02':
        bivariate_analysis_numerical_numerical(year2, 'NDVI02', col)

"""##### **NDVI03 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' :
        bivariate_analysis_numerical_numerical(year2, 'NDVI03', col)

"""##### **NDVI04 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04':
        bivariate_analysis_numerical_numerical(year2, 'NDVI04', col)

"""##### **NDVI05 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05':
        bivariate_analysis_numerical_numerical(year2, 'NDVI05', col)

"""##### **NDVI06 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06':
        bivariate_analysis_numerical_numerical(year2, 'NDVI06', col)

"""##### **NDVI07 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07':
        bivariate_analysis_numerical_numerical(year2, 'NDVI07', col)

"""##### **NDVI08 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08':
        bivariate_analysis_numerical_numerical(year2, 'NDVI08', col)

"""##### **NDVI09 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08' and col!= 'NDVI09':
        bivariate_analysis_numerical_numerical(year2, 'NDVI09', col)

"""##### **NDVI10 with all other NDVI values**"""

for col in year2.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08' and col != 'NDVI09' and col != 'NDVI10':
        bivariate_analysis_numerical_numerical(year2, 'NDVI10', col)

"""##### **NDVI11 with all other NDVI values**"""

bivariate_analysis_numerical_numerical(year2, 'NDVI11', 'NDVI12')

"""#### **4.2.2.3 Second Type: Numerical-Categorical**"""

# all numerical features with categorical
for col in year2.columns:
    if year2[col].dtype == 'float64':
        bivariate_analysis_numerical_categorical(year2, col, 'Class')

"""#### **4.2.2.4 Third Type: Categorical-Categoricals**

**no features for this combination avaiable for analysis**

## **4.3 Exploratory Data Analysis For Year 3**

### **4.3.1 Univariate Analysis**

Univariate analysis focuses on analyzing each feature in the dataset independently.


*   **Distribution analysis:** The distribution of each feature is examined to identify its shape, central tendency, and dispersion
*   **Identifying potential issues:** Univariate analysis helps in identifying potential problems with the data such as outliers, skewness and missing values

#### **4.3.1.1 Steps of doing Univariate Analysis on Numerical columns**

**Descriptive Statistics:** Compute basic summary statistics for the column, such as mean, median, mode, standard deviation, range, and quartiles. These statistics give a general understanding of the distribution of the data and can help identify skewness or outliers.

**Visualizations:** Create visualizations to explore the distribution of the data. Some common visualizations for numerical data include histograms, box plots, and density plots. These visualizations provide a visual representation of the distribution of the data and can help identify skewness an outliers.

**Data Imbalances**

**Identifying Outliers:** Identify and examine any outliers in the data. Outliers can be identified using visualizations. It is important to determine whether the outliers are due to measurement errors, data entry errors, or legitimate differences in the data, and to decide whether to include or exclude them from the analysis.

**Skewness:** Check for skewness in the data and consider transforming the data or using robust statistical methods that are less sensitive to skewness, if necessary.

**Conclusion:** Summarize the findings of the EDA and make decisions about how to proceed with further analysis.

#### **4.3.1.2 Numerical Variables**
"""

for col in year3.columns:
    if year3[col].dtype == 'float64':
        analyze_numerical_feature(year3, col)

"""#### **4.3.1.3 Categorical Variables**"""

analyze_categorical_feature(year3, 'Class')

"""### **4.3.2 Bivariate Analysis**

#### **4.3.2.1 Steps of doing Bivariate Analysis**

- Select 2 cols
- Understand type of relationship
    1. **Numerical - Numerical**<br>
        a. You can plot graphs like scatterplot(regression plots), 2D histplot, 2D KDEplots<br>
        b. Check correlation coefficent to check linear relationship
    2. **Numerical - Categorical** - create visualizations that compare the distribution of the numerical data across different categories of the categorical data.<br>
        a. You can plot graphs like barplot, boxplot, kdeplot violinplot even scatterplots. Scatterplots are not as much beneficial<br>
    3. **Categorical - Categorical**<br>
        a. You can create cross-tabulations or contingency tables that show the distribution of values in one categorical column, grouped by the values in the other categorical column.<br>
        b. You can plots like heatmap, stacked barplots, treemaps

#### **4.3.2.2 First Type: Numerical-Numerical**

##### **NDVI01 with all other NDVI values**
"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01':
        bivariate_analysis_numerical_numerical(year3, 'NDVI01', col)

"""##### **NDVI02 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02':
        bivariate_analysis_numerical_numerical(year3, 'NDVI02', col)

"""##### **NDVI03 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' :
        bivariate_analysis_numerical_numerical(year3, 'NDVI03', col)

"""##### **NDVI04 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04':
        bivariate_analysis_numerical_numerical(year3, 'NDVI04', col)

"""##### **NDVI05 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05':
        bivariate_analysis_numerical_numerical(year3, 'NDVI05', col)

"""##### **NDVI06 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06':
        bivariate_analysis_numerical_numerical(year3, 'NDVI06', col)

"""##### **NDVI07 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07':
        bivariate_analysis_numerical_numerical(year3, 'NDVI07', col)

"""##### **NDVI08 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08':
        bivariate_analysis_numerical_numerical(year3, 'NDVI08', col)

"""##### **NDVI09 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08' and col!= 'NDVI09':
        bivariate_analysis_numerical_numerical(year3, 'NDVI09', col)

"""##### **NDVI10 with all other NDVI values**"""

for col in year3.columns:
    if col.startswith('NDVI') and col != 'NDVI01' and col != 'NDVI02' and col != 'NDVI03' and col != 'NDVI04' and col != 'NDVI05' and col != 'NDVI06' and col != 'NDVI07' and col != 'NDVI08' and col != 'NDVI09' and col != 'NDVI10':
        bivariate_analysis_numerical_numerical(year3, 'NDVI10', col)

"""##### **NDVI11 with all other NDVI values**"""

bivariate_analysis_numerical_numerical(year3, 'NDVI11', 'NDVI12')

"""#### **4.3.2.3 Second Type: Numerical-Categorical**"""

# all numerical features with categorical
for col in year3.columns:
    if year3[col].dtype == 'float64':
        bivariate_analysis_numerical_categorical(year3, col, 'Class')

"""#### **4.3.2.4 Third Type: Categorical-Categoricals**

**no features for this combination avaiable for analysis**

# **5. Time Series Plot Analysis**
"""

global ndvi_columns
ndvi_columns = ['NDVI01', 'NDVI02', 'NDVI03', 'NDVI04', 'NDVI05', 'NDVI06', 'NDVI07', 'NDVI08', 'NDVI09', 'NDVI10', 'NDVI11', 'NDVI12']
seasonal_time_series_rice(ndvi_columns, title="Rice NDVI Seasonal Time Series (2021-2023)")

seasonal_time_series_cotton(ndvi_columns, title="Cotton NDVI Seasonal Time Series (2021-2023)")

"""# **6. Investigation Of Correlation Matrix**

## **6.1 Correlation Matrix For Year 1**
"""

# Generate a correlation matrix
corr = year1.corr()
# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(20, 20))
# masking the upper triagle repleated values
masking = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, mask= masking, cmap='Blues')
plt.show()

"""## **6.2 Correlation Matrix For Year 2**"""

# Generate a correlation matrix
corr = year2.corr()
# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(20, 20))
# masking the upper triagle repleated values
masking = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, mask= masking, cmap='Blues')
plt.show()

"""## **6.3 Correlation Matrix For Year 3**"""

# Generate a correlation matrix
corr = year3.corr()
# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(20, 20))
# masking the upper triagle repleated values
masking = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, mask= masking, cmap='Blues')
plt.show()

"""#  **7. Preprocessing**

Making Copies Of Data
"""

# copy of year1
year1_copy = year1.copy()
# copy of year2
year2_copy = year2.copy()
# copy of year3
year3_copy = year3.copy()

year1= year1_copy.copy()

"""## **7.1 Preprocessing For Year 1**

### **7.1.1 Missing Values**

checkig the missing values for year1
"""

check_missing_values(year1)

"""### **7.1.2 Duplicated And Low Variance Data**"""

inspect_data(year1)

check_duplicates(year1)

calculate_duplicate_percentage(year1)

"""**Checking The Nature Of Duplicates**"""

duplicates = year1[year1.duplicated(subset= ndvi_columns, keep=False)]
duplicates

"""Before Making Some Decision About The Duplicates, we need to consider the following questions.
1. Are they true duplicates?
2. Could they result from data collection artifacts?
3. Do they reflect real-world phenomena?


---


Analyzing the duplicates on the basis of these questions to reach a good decision for the duplicated values.

**1. Are they true duplicates?**

Yes, rows like 185 and 186, or 386 and 387, are true duplicates because all 12 NDVI features and the class label are identical. With no additional features like timestamps or locations, these rows are indistinguishable and likely represent redundant entries in the dataset


---

**2. Could they result from data collection artifacts?**

Considering only NDVI values as features, the duplicates might arise from:

* **Repeated Measurements:** The same NDVI readings could have been recorded multiple times during data collection.

* **Dataset Compilation:** The dataset might have been compiled from multiple sources, accidentally including repeated rows.

* **Sensor Artifacts:** While rounding isn't evident due to high precision, duplicates might result from overlapping observations (e.g., multiple readings from the same area in a field).

---


**3. Do they reflect real-world phenomena?**
In the absence of spatial or temporal metadata, duplicates could represent:

* **Uniformity in Vegetation Health:** If specific areas of a cotton field have highly uniform conditions, similar NDVI values across all 12 features could occur. However, exact duplicates across all features are less likely to reflect natural variability.

* **Artifacts from Feature Engineering:** If NDVI values were derived (e.g., aggregated averages or interpolated), duplicates might appear due to the data generation process.

---

**Decision About Duplicates:**
* Considering the facts stated above and the fact that cotton data is more than the rice, we have decided to drop these duplicates that consists of 537 rows of the dataset of 3302 rows.
* **Plus point:** All the duplicates are from the cotton class 0, dropping the duplicates will automatically address the class imbalance to some extent.

**Before Dropping Duplicates**
"""

analyze_categorical_feature(year1, 'Class')

"""**Dropping Duplicates**"""

# dropping duplicats
year1 = year1.drop_duplicates()

"""**After Dropping Duplicates**"""

analyze_categorical_feature(year1, 'Class')

"""**Conclusion:**
* there is still a large imbalance so next step is to deal with class imbalance.

### **7.1.5 Addressing Class Imbalance(moved up)**

#### **7.1.5.1 Analyze class imbalance**
"""

analyze_categorical_feature(year1, 'Class')

"""#### **7.1.5.2 Decide on the desired level of balance**

**here we are prefering exact balance to partial balance**
"""

samples = calculate_samples_needed(year1, 'Class', 1, balance_ratio=1.0)

def calculate_samples_needed(df, target_column, minority_class_label, balance_ratio=1.0):
    """
    Calculate the number of samples needed for the minority class to achieve a desired balance ratio.

    Args:
        df (pd.DataFrame): The dataset.
        target_column (str): The name of the target column.
        minority_class_label (int): The label of the minority class.
        balance_ratio (float): The desired ratio of minority class to majority class.

    Returns:
        int: The number of samples needed for the minority class.
    """
    minority_class_count = df[df[target_column] == minority_class_label].shape[0]
    majority_class_count = df[df[target_column] != minority_class_label].shape[0]
    samples_needed = int(majority_class_count * balance_ratio - minority_class_count)
    print(f"The number of samples needed for the minority class: {samples_needed}")
    return samples_needed

"""#### **7.1.5.3 Random Oversampling**"""

year1_random_oversampling = random_oversample_dataset(year1, 'Class')

year1_random_oversampling.shape

analyze_categorical_feature(year1_random_oversampling, 'Class')

"""#### **7.1.5.4 Random Undersampling**"""

year1_random_undersampling = random_undersample_dataset(year1, 'Class')

year1_random_undersampling.shape

analyze_categorical_feature(year1_random_undersampling, 'Class')

"""#### **7.1.5.5 SMOTE**"""

year1_smote = smote_oversample_dataset(year1, 'Class')

year1_smote.shape

analyze_categorical_feature(year1_smote, 'Class')

"""### **7.1.3 Outliers Detection**

#### **z-score**
"""

year1.shape

outliers = detect_outliers_zscore(year1)

"""**dropping outliers**"""

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Local Outliers Detection**"""

year1 = year1_copy.copy()

outliers = detect_outliers_lof(year1)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Isolation Forest**"""

year1 = year1_copy.copy()

outliers = detect_outliers_isolation_forest(year1)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Final Outliers Removal Using z-score**"""

year1_random_oversampling_copy = year1_random_oversampling.copy()
outliers = detect_outliers_zscore(year1_random_oversampling)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year1_random_oversampling = year1_random_oversampling.drop(outliers.index)

print(year1_random_oversampling_copy.shape)
print(year1_random_oversampling.shape)

year1_random_undersampling_copy = year1_random_undersampling.copy()
outliers = detect_outliers_zscore(year1_random_undersampling)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year1_random_undersampling = year1_random_undersampling.drop(outliers.index)

print(year1_random_undersampling_copy.shape)
print(year1_random_undersampling.shape)

year1_smote_copy = year1_smote.copy()
outliers = detect_outliers_zscore(year1_smote)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year1_smote = year1_smote.drop(outliers.index)

print(year1_smote_copy.shape)
print(year1_smote.shape)

"""### **7.1.4 Features Scaling(moved down)**"""

year1.shape

# printing the skewness of all features in tabular form
skewness = year1.skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

"""#### **Reasoning and Workflow for Applying Power Transformation + Standardization**

---

##### **1. Reasoning**

1. **Purpose of Power Transformation**:
   - Reduces skewness in the data for all features.
   - Handles both positive and negative skewness effectively (using Yeo-Johnson).
   - Creates feature distributions more suitable for machine learning models.

2. **Purpose of Standardization (Z-score)**:
   - Scales features to have a mean of 0 and standard deviation of 1.
   - Ensures consistent feature scales, especially important for distance-based models like SVM.

3. **Impact on Models**:
   - **Tree-Based Models (XGBoost, Bagging, Random Forest)**:
     - Insensitive to feature scaling, but benefit slightly from reduced skewness.
     - Standardization is not strictly needed but doesnt harm performance.
   - **SVM**:
     - Highly sensitive to feature scales and distribution.
     - Both power transformation and standardization improve model performance by ensuring consistent and Gaussian-like feature distributions.


---

##### **2. Workflow**

1. **Step 1: Apply Power Transformation**  
   - **Reason**: Reduces skewness for all features, making the data more symmetric and stable.
   - **Method**: Use Yeo-Johnson transformation (supports both positive and negative values).

2. **Step 2: Apply Standardization**  
   - **Reason**: Scales features to a mean of 0 and a standard deviation of 1, essential for SVM.
   - **Method**: Use StandardScaler for consistent scaling across all features.


3. **Step 3: Train Models with Preprocessed Data**
   - Use the preprocessed dataset  for training all models.
   - **Workflow for Models**:
     - **Tree-Based Models (XGBoost, Bagging, Random Forest)**:
       - Benefit from reduced skewness, though scaling is not critical.
     - **SVM**:
       - Both steps are crucial for optimal performance.

---

This approach ensures a robust and consistent preprocessing pipeline for your models.

#### **Oversampled Data Scaling**
"""

year1_random_oversampling_X = year1_random_oversampling.drop('Class', axis=1)
year1_random_oversampling_y = year1_random_oversampling['Class']

year1_random_oversampling_X_scaled = scaling_features(year1_random_oversampling_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year1_random_oversampling_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year1_RO_X_scaled = pd.DataFrame(year1_random_oversampling_X_scaled)
year1_RO_y = pd.DataFrame(year1_random_oversampling_y)

year1_RO_scaled = pd.concat([year1_RO_X_scaled.reset_index(drop=True),
                          year1_RO_y.reset_index(drop=True)], axis=1)

year1_RO_scaled.shape

year1_RO_scaled.head()

"""#### **Undersampled Data Scaling**"""

year1_random_undersampling_X = year1_random_undersampling.drop('Class', axis=1)
year1_random_undersampling_y = year1_random_undersampling['Class']

year1_random_undersampling_X_scaled = scaling_features(year1_random_undersampling_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year1_random_undersampling_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year1_RU_X_scaled = pd.DataFrame(year1_random_undersampling_X_scaled)
year1_RU_y = pd.DataFrame(year1_random_undersampling_y)

year1_RU_scaled = pd.concat([year1_RU_X_scaled.reset_index(drop=True),
                          year1_RU_y.reset_index(drop=True)], axis=1)

year1_RU_scaled.shape

year1_RU_scaled.head()

"""#### **SMOTE Data Scaling**"""

year1_SMOTE_X = year1_smote.drop('Class', axis=1)
year1_SMOTE_y = year1_smote['Class']

year1_SMOTE_X_scaled = scaling_features(year1_SMOTE_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year1_SMOTE_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year1_SMOTE_X_scaled = pd.DataFrame(year1_SMOTE_X_scaled)
year1_SMOTE_y = pd.DataFrame(year1_SMOTE_y)

year1_SMOTE_scaled = pd.concat([year1_SMOTE_X_scaled.reset_index(drop=True),
                          year1_SMOTE_y.reset_index(drop=True)], axis=1)

year1_SMOTE_scaled.shape

year1_SMOTE_scaled.head()

"""### **7.1.7 Visualizations**"""

scatter_plot(year1_RO_scaled, 'Class')

scatter_plot(year1_RU_scaled, 'Class')

scatter_plot(year1_SMOTE_scaled, 'Class')

"""### **7.1.8 Feature Engineering & Selection**

* Feature importance derived from ensemble methods, such as Random Forest and XGBoost, can be a powerful tool for feature selection in Support Vector Machines (SVM).

* Ensemble methods inherently provide feature importance scores by evaluating the contribution of each feature to the model's predictions, which is particularly useful in identifying the most informative features in high-dimensional datasets.

* These scores can be used to rank features and select the top-k most important ones, effectively reducing irrelevant or redundant features.

* By leveraging feature importance from ensemble models, we can streamline the input to SVM, improving its performance by mitigating overfitting and reducing computational complexity.

* This approach combines the interpretability of ensemble methods with the robust classification capabilities of SVM, ensuring that only the most critical features are used for training.
"""



"""### **7.1.9 Validation Split**"""

# For year1_random_oversampling
X_RO = year1_RO_scaled.drop('Class', axis=1)  # Features
y_RO = year1_RO_scaled['Class']  # Target

# Split into train and test sets (80-20 split)
year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test = train_test_split(X_RO, y_RO, test_size=0.2, random_state=42)

# For year1_random_undersampling
X_RU = year1_RU_scaled.drop('Class', axis=1)  # Features
y_RU = year1_RU_scaled['Class']  # Taingrget

# Split into train and test sets (80-20 split)
year1_RU_X_train, year1_RU_X_test, year1_RU_y_train, year1_RU_y_test = train_test_split(X_RU, y_RU, test_size=0.2, random_state=42)

# For year1_smote_augmented_data
X_SMOTE = year1_SMOTE_scaled.drop('Class', axis=1)  # Features
y_SMOTE = year1_SMOTE_scaled['Class']  # Target

# Split into train and test sets (80-20 split)
year1_SMOTE_X_train, year1_SMOTE_X_test, year1_SMOTE_y_train, year1_SMOTE_y_test = train_test_split(X_SMOTE, y_SMOTE, test_size=0.2, random_state=42)

"""## **7.2 Preprocessing For Year 2**

### **7.2.1 Missing Values**

checkig the missing values for year2
"""

check_missing_values(year2)

"""### **7.2.2 Duplicated And Low Variance Data**"""

inspect_data(year2)

check_duplicates(year2)

calculate_duplicate_percentage(year2)

"""**Checking The Nature Of Duplicates**"""

duplicates = year2[year2.duplicated(subset= ndvi_columns, keep=False)]
duplicates

"""**Before Dropping Duplicates**"""

analyze_categorical_feature(year2, 'Class')

"""**Dropping Duplicates**"""

# dropping duplicats
year2 = year2.drop_duplicates()

"""**After Dropping Duplicates**"""

analyze_categorical_feature(year2, 'Class')

"""### **7.2.5 Addressing Class Imbalance(moved up)**

#### **7.2.5.1 Analyze class imbalance**
"""

analyze_categorical_feature(year2, 'Class')

"""#### **7.2.5.2 Decide on the desired level of balance**

**here we are prefering exact balance to partial balance**
"""

samples = calculate_samples_needed(year2, 'Class', 1, balance_ratio=1.0)

"""#### **7.2.5.3 Random Oversampling**"""

year2_random_oversampling = random_oversample_dataset(year2, 'Class')

year2_random_oversampling.shape

analyze_categorical_feature(year2_random_oversampling, 'Class')

"""#### **7.2.5.4 Random Undersampling**"""

year2_random_undersampling = random_undersample_dataset(year2, 'Class')

year2_random_undersampling.shape

analyze_categorical_feature(year2_random_undersampling, 'Class')

"""#### **7.2.5.5 SMOTE**"""

year2_smote = smote_oversample_dataset(year2, 'Class')

year2_smote.shape

analyze_categorical_feature(year2_smote, 'Class')

"""### **7.2.3 Outliers Detection**

#### **z-score**
"""

year2.shape

year2['Class'].value_counts()

outliers = detect_outliers_zscore(year2)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Local Outliers Detection**"""

year2 = year2_copy.copy()

outliers = detect_outliers_lof(year2)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Isolation Forest**"""

year2 = year2_copy.copy()

outliers = detect_outliers_isolation_forest(year2)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Final Outliers Removal Using z-score**"""

year2_random_oversampling_copy = year2_random_oversampling.copy()
outliers = detect_outliers_zscore(year2_random_oversampling)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year2_random_oversampling = year2_random_oversampling.drop(outliers.index)

print(year2_random_oversampling_copy.shape)
print(year2_random_oversampling.shape)

year2_random_undersampling_copy = year2_random_undersampling.copy()
outliers = detect_outliers_zscore(year2_random_undersampling)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year2_random_undersampling = year2_random_undersampling.drop(outliers.index)

print(year2_random_undersampling_copy.shape)
print(year2_random_undersampling.shape)

year2_smote_copy = year2_smote.copy()
outliers = detect_outliers_zscore(year2_smote)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year2_smote = year2_smote.drop(outliers.index)

print(year2_smote_copy.shape)
print(year2_smote.shape)

"""### **7.2.4 Features Scaling(moved down)**"""

year2.shape

# printing the skewness of all features in tabular form
skewness = year2.skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

"""#### **Reasoning and Workflow for Applying Power Transformation + Standardization**

---

##### **1. Reasoning**

1. **Purpose of Power Transformation**:
   - Reduces skewness in the data for all features.
   - Handles both positive and negative skewness effectively (using Yeo-Johnson).
   - Creates feature distributions more suitable for machine learning models.

2. **Purpose of Standardization (Z-score)**:
   - Scales features to have a mean of 0 and standard deviation of 1.
   - Ensures consistent feature scales, especially important for distance-based models like SVM.

3. **Impact on Models**:
   - **Tree-Based Models (XGBoost, Bagging, Random Forest)**:
     - Insensitive to feature scaling, but benefit slightly from reduced skewness.
     - Standardization is not strictly needed but doesnt harm performance.
   - **SVM**:
     - Highly sensitive to feature scales and distribution.
     - Both power transformation and standardization improve model performance by ensuring consistent and Gaussian-like feature distributions.


---

##### **2. Workflow**

1. **Step 1: Apply Power Transformation**  
   - **Reason**: Reduces skewness for all features, making the data more symmetric and stable.
   - **Method**: Use Yeo-Johnson transformation (supports both positive and negative values).

2. **Step 2: Apply Standardization**  
   - **Reason**: Scales features to a mean of 0 and a standard deviation of 1, essential for SVM.
   - **Method**: Use StandardScaler for consistent scaling across all features.


3. **Step 3: Train Models with Preprocessed Data**
   - Use the preprocessed dataset  for training all models.
   - **Workflow for Models**:
     - **Tree-Based Models (XGBoost, Bagging, Random Forest)**:
       - Benefit from reduced skewness, though scaling is not critical.
     - **SVM**:
       - Both steps are crucial for optimal performance.

---

This approach ensures a robust and consistent preprocessing pipeline for your models.

**Oversampled Data Scaling**
"""

year2_random_oversampling_X = year2_random_oversampling.drop('Class', axis=1)
year2_random_oversampling_y = year2_random_oversampling['Class']

year2_random_oversampling_X_scaled = scaling_features(year2_random_oversampling_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year2_random_oversampling_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year2_RO_X_scaled = pd.DataFrame(year2_random_oversampling_X_scaled)
year2_RO_y = pd.DataFrame(year2_random_oversampling_y)

year2_RO_scaled = pd.concat([year2_RO_X_scaled.reset_index(drop=True),
                          year2_RO_y.reset_index(drop=True)], axis=1)

year2_RO_scaled.shape

year2_RO_scaled.head()

"""**Undersampled Data Scaling**"""

year2_random_undersampling_X = year2_random_undersampling.drop('Class', axis=1)
year2_random_undersampling_y = year2_random_undersampling['Class']

year2_random_undersampling_X_scaled = scaling_features(year2_random_undersampling_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year2_random_undersampling_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year2_RU_X_scaled = pd.DataFrame(year2_random_undersampling_X_scaled)
year2_RU_y = pd.DataFrame(year2_random_undersampling_y)

year2_RU_scaled = pd.concat([year2_RU_X_scaled.reset_index(drop=True),
                          year2_RU_y.reset_index(drop=True)], axis=1)

year2_RU_scaled.shape

year2_RU_scaled.head()

"""**SMOTE Data Scaling**"""

year2_SMOTE_X = year2_smote.drop('Class', axis=1)
year2_SMOTE_y = year2_smote['Class']

year2_SMOTE_X_scaled = scaling_features(year2_SMOTE_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year2_SMOTE_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year2_SMOTE_X_scaled = pd.DataFrame(year2_SMOTE_X_scaled)
year2_SMOTE_y = pd.DataFrame(year2_SMOTE_y)

year2_SMOTE_scaled = pd.concat([year2_SMOTE_X_scaled.reset_index(drop=True),
                          year2_SMOTE_y.reset_index(drop=True)], axis=1)

year2_SMOTE_scaled.shape

year2_SMOTE_scaled.head()

"""### **7.2.7 Visualizations**"""

scatter_plot(year2_RO_scaled, 'Class')

scatter_plot(year2_RU_scaled, 'Class')

scatter_plot(year2_SMOTE_scaled, 'Class')

"""### **7.2.8 Feature Engineering & Selection**

* Feature importance derived from ensemble methods, such as Random Forest and XGBoost, can be a powerful tool for feature selection in Support Vector Machines (SVM).

* Ensemble methods inherently provide feature importance scores by evaluating the contribution of each feature to the model's predictions, which is particularly useful in identifying the most informative features in high-dimensional datasets.

* These scores can be used to rank features and select the top-k most important ones, effectively reducing irrelevant or redundant features.

* By leveraging feature importance from ensemble models, we can streamline the input to SVM, improving its performance by mitigating overfitting and reducing computational complexity.

* This approach combines the interpretability of ensemble methods with the robust classification capabilities of SVM, ensuring that only the most critical features are used for training.

### **7.2.9 Validation Split**
"""

# For year1_random_oversampling
X_RO = year2_RO_scaled.drop('Class', axis=1)  # Features
y_RO = year2_RO_scaled['Class']  # Target

# Split into train and test sets (80-20 split)
year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test = train_test_split(X_RO, y_RO, test_size=0.2, random_state=42)

# For year1_random_undersampling
X_RU = year2_RU_scaled.drop('Class', axis=1)  # Features
y_RU = year2_RU_scaled['Class']  # Taingrget

# Split into train and test sets (80-20 split)
year2_RU_X_train, year2_RU_X_test, year2_RU_y_train, year2_RU_y_test = train_test_split(X_RU, y_RU, test_size=0.2, random_state=42)

# For year1_smote_augmented_data
X_SMOTE = year2_SMOTE_scaled.drop('Class', axis=1)  # Features
y_SMOTE = year2_SMOTE_scaled['Class']  # Target

# Split into train and test sets (80-20 split)
year2_SMOTE_X_train, year2_SMOTE_X_test, year2_SMOTE_y_train, year2_SMOTE_y_test = train_test_split(X_SMOTE, y_SMOTE, test_size=0.2, random_state=42)

"""## **7.3 Preprocessing For Year 3**

### **7.3.1 Missing Values**

checkig the missing values for year3
"""

check_missing_values(year3)

"""### **7.3.2 Duplicated And Low Variance Data**"""

inspect_data(year3)

check_duplicates(year3)

calculate_duplicate_percentage(year3)

"""**Checking The Nature Of Duplicates**"""

duplicates = year1[year3.duplicated(subset= ndvi_columns, keep=False)]
duplicates

"""**Before Dropping Duplicates**"""

analyze_categorical_feature(year3, 'Class')

"""**Dropping Duplicates**"""

# dropping duplicats
year3 = year3.drop_duplicates()

"""**Before Dropping Duplicates**"""

analyze_categorical_feature(year3, 'Class')

"""### **7.3.5 Addressing Class Imbalance(moved up)**

#### **7.3.5.1 Analyze class imbalance**
"""

analyze_categorical_feature(year3, 'Class')

"""#### **7.3.5.2 Decide on the desired level of balance**

**here we are prefering exact balance to partial balance**
"""

samples = calculate_samples_needed(year3, 'Class', 1, balance_ratio=1.0)

"""#### **7.2.5.3 Random Oversampling**"""

year3_random_oversampling = random_oversample_dataset(year3, 'Class')

year3_random_oversampling.shape

analyze_categorical_feature(year3_random_oversampling, 'Class')

"""#### **7.2.5.4 Random Undersampling**"""

year3_random_undersampling = random_undersample_dataset(year3, 'Class')

year3_random_undersampling.shape

analyze_categorical_feature(year3_random_undersampling, 'Class')

"""#### **7.2.5.5 SMOTE**"""

year3_smote = smote_oversample_dataset(year3, 'Class')

year3_smote.shape

analyze_categorical_feature(year3_smote, 'Class')

"""### **7.3.3 Outliers Detection**

#### **z-score**
"""

year3.shape

year3['Class'].value_counts()

outliers = detect_outliers_zscore(year3)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Local Outliers Detection**"""

year3 = year3_copy.copy()

outliers = detect_outliers_lof(year3)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Isolation Forest**"""

year3 = year3_copy.copy()

outliers = detect_outliers_isolation_forest(year3)

# count the rows with  0 and row with 1
outliers['Class'].value_counts()

"""#### **Final Outliers Removal Using z-score**"""

year3_random_oversampling_copy = year3_random_oversampling.copy()
outliers = detect_outliers_zscore(year3_random_oversampling)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year3_random_oversampling = year3_random_oversampling.drop(outliers.index)

print(year3_random_oversampling_copy.shape)
print(year3_random_oversampling.shape)

year3_random_undersampling_copy = year3_random_undersampling.copy()
outliers = detect_outliers_zscore(year3_random_undersampling)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year3_random_undersampling = year3_random_undersampling.drop(outliers.index)

print(year3_random_undersampling_copy.shape)
print(year3_random_undersampling.shape)

year3_smote_copy = year3_smote.copy()
outliers = detect_outliers_zscore(year3_smote)
# count the rows with  0 and row with 1
outliers['Class'].value_counts()

year3_smote = year3_smote.drop(outliers.index)

print(year3_smote_copy.shape)
print(year3_smote.shape)

"""### **7.2.4 Features Scaling(moved down)**"""

year3.shape

# printing the skewness of all features in tabular form
skewness = year3.skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

"""#### **Reasoning and Workflow for Applying Power Transformation + Standardization**

---

##### **1. Reasoning**

1. **Purpose of Power Transformation**:
   - Reduces skewness in the data for all features.
   - Handles both positive and negative skewness effectively (using Yeo-Johnson).
   - Creates feature distributions more suitable for machine learning models.

2. **Purpose of Standardization (Z-score)**:
   - Scales features to have a mean of 0 and standard deviation of 1.
   - Ensures consistent feature scales, especially important for distance-based models like SVM.

3. **Impact on Models**:
   - **Tree-Based Models (XGBoost, Bagging, Random Forest)**:
     - Insensitive to feature scaling, but benefit slightly from reduced skewness.
     - Standardization is not strictly needed but doesnt harm performance.
   - **SVM**:
     - Highly sensitive to feature scales and distribution.
     - Both power transformation and standardization improve model performance by ensuring consistent and Gaussian-like feature distributions.


---

##### **2. Workflow**

1. **Step 1: Apply Power Transformation**  
   - **Reason**: Reduces skewness for all features, making the data more symmetric and stable.
   - **Method**: Use Yeo-Johnson transformation (supports both positive and negative values).

2. **Step 2: Apply Standardization**  
   - **Reason**: Scales features to a mean of 0 and a standard deviation of 1, essential for SVM.
   - **Method**: Use StandardScaler for consistent scaling across all features.


3. **Step 3: Train Models with Preprocessed Data**
   - Use the preprocessed dataset  for training all models.
   - **Workflow for Models**:
     - **Tree-Based Models (XGBoost, Bagging, Random Forest)**:
       - Benefit from reduced skewness, though scaling is not critical.
     - **SVM**:
       - Both steps are crucial for optimal performance.

---

This approach ensures a robust and consistent preprocessing pipeline for your models.

**Oversampled Data Scaling**
"""

year3_random_oversampling_X = year3_random_oversampling.drop('Class', axis=1)
year3_random_oversampling_y = year3_random_oversampling['Class']

year3_random_oversampling_X_scaled = scaling_features(year3_random_oversampling_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year3_random_oversampling_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year3_RO_X_scaled = pd.DataFrame(year3_random_oversampling_X_scaled)
year3_RO_y = pd.DataFrame(year3_random_oversampling_y)

year3_RO_scaled = pd.concat([year3_RO_X_scaled.reset_index(drop=True),
                          year3_RO_y.reset_index(drop=True)], axis=1)

year3_RO_scaled.shape

year3_RO_scaled.head()

"""**Undersampled Data Scaling**"""

year3_random_undersampling_X = year3_random_undersampling.drop('Class', axis=1)
year3_random_undersampling_y = year3_random_undersampling['Class']

year3_random_undersampling_X_scaled = scaling_features(year3_random_undersampling_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year3_random_undersampling_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year3_RU_X_scaled = pd.DataFrame(year3_random_undersampling_X_scaled)
year3_RU_y = pd.DataFrame(year3_random_undersampling_y)

year3_RU_scaled = pd.concat([year3_RU_X_scaled.reset_index(drop=True),
                          year3_RU_y.reset_index(drop=True)], axis=1)

year3_RU_scaled.shape

year3_RU_scaled.head()

"""**SMOTE Data Scaling**"""

year3_SMOTE_X = year3_smote.drop('Class', axis=1)
year3_SMOTE_y = year3_smote['Class']

year3_SMOTE_X_scaled = scaling_features(year3_SMOTE_X)

# recalculating skewness of year1_random_oversampling_X_scaled
skewness = pd.DataFrame(year3_SMOTE_X_scaled).skew()
skewness_df = pd.DataFrame(skewness, columns=['Skewness'])
skewness_df

year3_SMOTE_X_scaled = pd.DataFrame(year3_SMOTE_X_scaled)
year3_SMOTE_y = pd.DataFrame(year3_SMOTE_y)

year3_SMOTE_scaled = pd.concat([year3_SMOTE_X_scaled.reset_index(drop=True),
                          year3_SMOTE_y.reset_index(drop=True)], axis=1)

year3_SMOTE_scaled.shape

year3_SMOTE_scaled.head()

"""### **7.2.7 Visualizations**"""

scatter_plot(year3_RO_scaled, 'Class')

scatter_plot(year3_RU_scaled, 'Class')

scatter_plot(year3_SMOTE_scaled, 'Class')

"""### **7.2.8 Feature Engineering & Selection**

* Feature importance derived from ensemble methods, such as Random Forest and XGBoost, can be a powerful tool for feature selection in Support Vector Machines (SVM).

* Ensemble methods inherently provide feature importance scores by evaluating the contribution of each feature to the model's predictions, which is particularly useful in identifying the most informative features in high-dimensional datasets.

* These scores can be used to rank features and select the top-k most important ones, effectively reducing irrelevant or redundant features.

* By leveraging feature importance from ensemble models, we can streamline the input to SVM, improving its performance by mitigating overfitting and reducing computational complexity.

* This approach combines the interpretability of ensemble methods with the robust classification capabilities of SVM, ensuring that only the most critical features are used for training.

### **7.2.9 Validation Split**
"""

# For year1_random_oversampling
X_RO = year3_RO_scaled.drop('Class', axis=1)  # Features
y_RO = year3_RO_scaled['Class']  # Target

# Split into train and test sets (80-20 split)
year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test = train_test_split(X_RO, y_RO, test_size=0.2, random_state=42)

# For year1_random_undersampling
X_RU = year3_RU_scaled.drop('Class', axis=1)  # Features
y_RU = year3_RU_scaled['Class']  # Taingrget

# Split into train and test sets (80-20 split)
year3_RU_X_train, year3_RU_X_test, year3_RU_y_train, year3_RU_y_test = train_test_split(X_RU, y_RU, test_size=0.2, random_state=42)

# For year1_smote_augmented_data
X_SMOTE = year3_SMOTE_scaled.drop('Class', axis=1)  # Features
y_SMOTE = year3_SMOTE_scaled['Class']  # Target

# Split into train and test sets (80-20 split)
year3_SMOTE_X_train, year3_SMOTE_X_test, year3_SMOTE_y_train, year3_SMOTE_y_test = train_test_split(X_SMOTE, y_SMOTE, test_size=0.2, random_state=42)

"""# **8. Hyperparameters Tuning Of XGBoost, Bagging(Bootstrap Aggregation), and Random Forest For Feature Selection Based On Their Importance**

## **Using Random Oversampling Technique**

### **Train on Year 1 and Year 2, Test on Year 3**

#### **XGBoost**
"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""### **Train on Year 2 and Year 3, Test on Year 1**

#### **XGBoost**
"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 2 and Year 3, Test on Year 1
# merge year1 and year2
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 2 and Year 3, Test on Year 1
# merge year1 and year2
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 2 and Year 3, Test on Year 1
# merge year1 and year2
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""### **Train on Year 3 and Year 1, Test on Year 2**

#### **XGBoost**
"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

# year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test
# year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test
# year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""## **Using Random Undersampling Technique**

### **Train on Year 1 and Year 2, Test on Year 3**

#### **XGBoost**
"""

# year1_RU_X_train, year1_RU_X_test, year1_RU_y_train, year1_RU_y_test
# year2_RU_X_train, year2_RU_X_test, year2_RU_y_train, year2_RU_y_test
# year3_RU_X_train, year3_RU_X_test, year3_RU_y_train, year3_RU_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

# year1_RU_X_train, year1_RU_X_test, year1_RU_y_train, year1_RU_y_test
# year2_RU_X_train, year2_RU_X_test, year2_RU_y_train, year2_RU_y_test
# year3_RU_X_train, year3_RU_X_test, year3_RU_y_train, year3_RU_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

# year1_RU_X_train, year1_RU_X_test, year1_RU_y_train, year1_RU_y_test
# year2_RU_X_train, year2_RU_X_test, year2_RU_y_train, year2_RU_y_test
# year3_RU_X_train, year3_RU_X_test, year3_RU_y_train, year3_RU_y_test


# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""### **Train on Year 2 and Year 3, Test on Year 1**

#### **XGBoost**
"""

# Train on Year 2 and Year 3, Test on Year 1
# merge year1 and year2
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

# Train on Year 2 and Year 3, Test on Year 1
# merge year1 and year2
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

# Train on Year 2 and Year 3, Test on Year 1
# merge year1 and year2
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""### **Train on Year 3 and Year 1, Test on Year 2**

#### **XGBoost**
"""

X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""## **Using SMOTE Technique**

### **Train on Year 1 and Year 2, Test on Year 3**

#### **XGBoost**
"""

# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

# Train on Year 1 and Year 2, Test on Year 3
# merge year1 and year2
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""### **Train on Year 2 and Year 3, Test on Year 1**

#### **XGBoost**
"""

X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

# write code for bagging using decision tree and the following parameters estimator__max_depth: None
# Max_samples: 1.0
# Max_features: 0.7
# N_estimators: 100

# Initialize the Decision Tree classifier
dt = DecisionTreeClassifier(max_depth=None)

# Initialize the Bagging Classifier
bagging_clf = BaggingClassifier(
    estimator=dt,
    n_estimators=100,         # Number of base estimators in the ensemble
    max_samples=1.0,         # Fraction of the dataset to draw as samples for training each base estimator
    max_features=0.7,        # Fraction of features to draw from the feature set
    random_state=42
)

# Fit the Bagging Classifier
bagging_clf.fit(X_train, y_train)

# Make predictions
y_pred = bagging_clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Plot Confusion Matrix
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#### **Random Forest**"""

X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **SVM**"""

X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

visualize_svm_results(X_train, X_test, y_train, y_test, best_model)

"""### **Train on Year 3 and Year 1, Test on Year 2**

#### **XGBoost**
"""

X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
best_model, best_params = xgb_grid_search(X_train, y_train)
metrics = xgb_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Bagging**"""

X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

best_model, best_params = bagging_grid_search(X_train, y_train)
metrics = bagging_model_training(X_train, X_test, y_train, y_test, best_model)

"""#### **Random Forest**"""

X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

best_model, best_params = random_forest_grid_search(X_train, y_train)
metrics = random_forest_model_training(X_train, X_test, y_train, y_test, best_model)



"""#### **SVM**"""

X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

best_model, best_params = svm_grid_search(X_train, y_train)

metrics = svm_model_training(X_train, X_test, y_train, y_test, best_model)

# Convert to DataFrames if necessary
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

# Ensure labels are in 1D format
y_train = y_train.ravel()
y_test = y_test.ravel()

# Call the function
visualize_svm_results(X_train, y_train, X_test, y_test, best_model)

"""# **9. Summary Of Grid Search**

### 1. **XGBoost**

- **Best Parameters**:  
  - **Learning Rate**: 0.1 or 0.2  
  - **Max Depth**: 7 to 10  
  - **Number of Estimators**: 200 to 300  
- **Performance**:  
  - XGBoost performed consistently well across different balancing techniques, with **Random Undersampling** achieving the highest stability and precision-recall balance.
  - Feature importance consistently ranked features like **NDVI05, NDVI06, and NDVI11** as highly significant.

### 2. **Bagging (Bootstrap Aggregation)**

- **Best Parameters**:  
  - **Base Estimator**: Decision Tree with **Max Depth**: None  
  - **Max Samples**: 1.0  
  - **Max Features**: 0.7  
  - **Number of Estimators**: 100  
- **Performance**:  
  - Bagging exhibited strong performance with **Random Undersampling** achieving the highest accuracy (up to 91%).  
  - Consistent feature importance rankings highlighted the relevance of **NDVI04, NDVI07, and NDVI11**.

### 3. **Random Forest**

- **Best Parameters**:  
  - **Max Depth**: None or 10  
  - **Min Samples Leaf**: 1 or 2  
  - **Number of Estimators**: 100 to 200  
- **Performance**:  
  - **Random Undersampling** yielded the best performance, achieving up to **91% accuracy**.  
  - Important features included **NDVI10, NDVI09, and NDVI11** across all splits and balancing methods.

### 4. **SVM (Support Vector Machine)**

- **Best Parameters**:  
  - **C**: 10  
  - **Kernel**: RBF (Radial Basis Function)  
  - **Gamma**: 'scale' or 'auto'  
- **Performance**:  
  - SVM showed moderate performance across all resampling techniques, with accuracy ranging between **67% and 84%**.  
  - The highest accuracy was achieved with **Random Oversampling**.

# **10. Summary Of Features**

### **Top Important Features**

Across all models and resampling techniques, the following NDVI features consistently ranked highest in importance:

1. **NDVI05**  Captures mid-season growth patterns, which are critical for distinguishing the phenological stages of rice and cotton.
2. **NDVI06**  Reflects peak vegetation health during the growing season, helping to differentiate between crop types based on their growth cycles.
3. **NDVI10**  Represents late-season growth and decline patterns, which vary significantly between rice and cotton.
4. **NDVI11**  Provides insights into the final stages of crop maturity, aiding in identifying the unique harvesting cycles of each crop.
5. **NDVI09**  Shows the transition phase between peak and late-season growth, capturing crop-specific variations.

These features consistently contributed to higher model accuracy and robust predictions, indicating their strong influence in distinguishing rice and cotton crops.

### **Less Important Features**

The following NDVI features consistently ranked low across all models and resampling techniques:

1. **NDVI00**  
2. **NDVI01**  
3. **NDVI02**  
4. **NDVI03**  

These early-season features showed limited differentiation between rice and cotton, making them less valuable for classification tasks. Their low importance suggests that they contribute less to the overall performance of the models and could potentially be considered for exclusion in future iterations to simplify the model.


---

# **11. Final Trained Models With All Features**

## **11.1 Random Oversampling**

### **11.1.1 XGBoost**
"""

import xgboost as xgb
#  XGBoost using best parameters
# 4.	Best parameters to use for further final training:
# o	Learning rate: 0.1 or 0.2
# o	Max_depth: 10 (frequent in high-performing models)
# o	N_estimators: 300 (most consistent value)

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

xgb_model = xgb.XGBClassifier(
    learning_rate=0.1,
    max_depth=10,
    n_estimators=300,
    random_state=42
)


xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

"""### **11.1.2 Bagging**"""

# Best Parameters for Bagging
# 	Base Estimator: Decision Tree with max_depth=None.
# 	Max Samples: 1.0 (Use all samples per tree).
# 	Max Features: 0.7 (Balanced performance with a subset of features).
# 	Number of Estimators (n_estimators): 100.

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=None),
    n_estimators=100,
    max_samples=1.0,
    max_features=0.7,
    random_state=42
)
bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

"""### **11.1.3 Random Forest**"""

# Best Parameters for Final Model:
# 	Max_depth: None (or 10 for regularization)
# 	Min_samples_leaf: 1 (or 2 for regularization)
# 	N_estimators: 100 to 200


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

random_forest_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_leaf=1,
    random_state=42
)
random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

"""### **11.1.4 SVM**"""

# Best Parameters for Final Model:
# 'C': 10
# 'gamma': 'scale', 'kernel': 'rbf'


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

SVM_model = SVC(
    C=10,
    gamma='scale',
    kernel='rbf',
    random_state=42
)
svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

"""## **11.2 Random Undersampling**

### **11.2.1 XGBoost**
"""

import xgboost as xgb
#  XGBoost using best parameters
# 4.	Best parameters to use for further final training:
# o	Learning rate: 0.1 or 0.2
# o	Max_depth: 10 (frequent in high-performing models)
# o	N_estimators: 300 (most consistent value)

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

xgb_model = xgb.XGBClassifier(
    learning_rate=0.05,
    max_depth=10,
    n_estimators=300,
    random_state=42
)


xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

"""### **11.2.2 Bagging**"""

# Best Parameters for Bagging
# estimator__max_depth: None
# Max_samples: 0.5
# Max_features: 1.0
# N_estimators: 100


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=None),
    n_estimators=100,
    max_samples=0.5,
    max_features=1.0,
    random_state=42
)
bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

"""### **11.2.3 Random Forest**"""

# Best Parameters for Final Model:
# Max_depth: 20
# Min_samples_leaf: 1
# N_estimators: 100



# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

random_forest_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=20,
    min_samples_leaf=1,
    random_state=42
)
random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test


random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

"""### **11.2.4 SVM**"""

# Best Parameters for Final Model:
# 'C': 10
# 'gamma': 'scale', 'kernel': 'rbf'


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

SVM_model = SVC(
    C=10,
    gamma='scale',
    kernel='rbf',
    random_state=42
)
svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

"""## **11.3 SMOTE**

### **11.3.1 XGBoost**
"""

import xgboost as xgb
#  XGBoost using best parameters
# 4.	Best parameters to use for further final training:
# o	Learning rate: 0.1 or 0.2
# o	Max_depth: 10 (frequent in high-performing models)
# o	N_estimators: 300 (most consistent value)

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

xgb_model = xgb.XGBClassifier(
    learning_rate=0.2,
    max_depth=10,
    n_estimators=300,
    random_state=42
)


xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

"""### **11.3.2 Bagging**"""

# Best Parameters for Bagging
# estimator__max_depth: None
# estimator__max_depth: None
# Max_samples: 1.0
# Max_features: 0.7
# N_estimators: 100



# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=None),
    n_estimators=100,
    max_samples=1.0,
    max_features=0.7,
    random_state=42
)
bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

"""### **11.3.3 Random Forest**"""

# Best Parameters for Final Model:
# Max_depth: None
# Min_samples_leaf: 1
# N_estimators: 100

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

random_forest_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_leaf=1,
    random_state=42
)
random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test


random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

"""### **11.3.4 SVM**"""

# Best Parameters for Final Model:
# 'C': 10
# 'gamma': 'scale', 'kernel': 'rbf'


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

SVM_model = SVC(
    C=10,
    gamma='auto',
    kernel='rbf',
    random_state=42
)
svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

"""# **12. Final Trained Models With Reduced Feature Space**"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd

# Assuming you have already loaded and preprocessed your data for each year

def apply_pca(df, n_components=None, explained_variance_ratio=None, plot_variance=True):
    """
    Perform PCA for dimensionality reduction.

    Parameters:
    - df: DataFrame, the input data (will use all numerical columns)
    - n_components: int, the number of principal components to retain (optional)
    - explained_variance_ratio: float, the desired explained variance ratio (optional)
    - plot_variance: bool, whether to plot the explained variance ratio (default is True)

    Returns:
    - reduced_df: DataFrame, the transformed data with principal components
    - pca: fitted PCA model
    """
    # Select numerical columns
    num_columns = df.select_dtypes(include=['number']).columns

    # Scale the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[num_columns])

    # Initialize PCA
    if explained_variance_ratio:
        pca = PCA(n_components=explained_variance_ratio)
    else:
        pca = PCA(n_components=n_components)

    # Fit and transform the data
    X_pca = pca.fit_transform(X_scaled)

    # Create a DataFrame with principal components
    pc_columns = [f'PC{i+1}' for i in range(pca.n_components_)]
    reduced_df = pd.DataFrame(X_pca, columns=pc_columns, index=df.index)

    # Plot the explained variance ratio
    if plot_variance:
        plt.figure(figsize=(8, 5))
        plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')
        plt.xlabel('Number of Principal Components')
        plt.ylabel('Explained Variance Ratio')
        plt.title('Explained Variance by Principal Components')
        plt.grid(True)
        plt.show()


    print(f"Total explained variance: {sum(pca.explained_variance_ratio_):.2f}")

    return reduced_df, pca

import pandas as pd

# Define the number of components for PCA (keeping it consistent across all years)
n_components = 8

# === Year 1 ===
# Random Oversampling
X_RO1 = year1_RO_scaled.drop('Class', axis=1)
y_RO1 = year1_RO_scaled['Class']
X_RO1_pca, pca_model_RO1 = apply_pca(X_RO1, n_components)
year1_RO_X_train, year1_RO_X_test, year1_RO_y_train, year1_RO_y_test = train_test_split(X_RO1_pca, y_RO1, test_size=0.2, random_state=42)

# Random Undersampling
X_RU1 = year1_RU_scaled.drop('Class', axis=1)
y_RU1 = year1_RU_scaled['Class']
X_RU1_pca, pca_model_RU1 = apply_pca(X_RU1, n_components)
year1_RU_X_train, year1_RU_X_test, year1_RU_y_train, year1_RU_y_test = train_test_split(X_RU1_pca, y_RU1, test_size=0.2, random_state=42)

# SMOTE Augmentation
X_SMOTE1 = year1_SMOTE_scaled.drop('Class', axis=1)
y_SMOTE1 = year1_SMOTE_scaled['Class']
X_SMOTE1_pca, pca_model_SMOTE1 = apply_pca(X_SMOTE1, n_components)
year1_SMOTE_X_train, year1_SMOTE_X_test, year1_SMOTE_y_train, year1_SMOTE_y_test = train_test_split(X_SMOTE1_pca, y_SMOTE1, test_size=0.2, random_state=42)

# === Year 2 ===
# Random Oversampling
X_RO2 = year2_RO_scaled.drop('Class', axis=1)
y_RO2 = year2_RO_scaled['Class']
X_RO2_pca, pca_model_RO2 = apply_pca(X_RO2, n_components)
year2_RO_X_train, year2_RO_X_test, year2_RO_y_train, year2_RO_y_test = train_test_split(X_RO2_pca, y_RO2, test_size=0.2, random_state=42)

# Random Undersampling
X_RU2 = year2_RU_scaled.drop('Class', axis=1)
y_RU2 = year2_RU_scaled['Class']
X_RU2_pca, pca_model_RU2 = apply_pca(X_RU2, n_components)
year2_RU_X_train, year2_RU_X_test, year2_RU_y_train, year2_RU_y_test = train_test_split(X_RU2_pca, y_RU2, test_size=0.2, random_state=42)

# SMOTE Augmentation
X_SMOTE2 = year2_SMOTE_scaled.drop('Class', axis=1)
y_SMOTE2 = year2_SMOTE_scaled['Class']
X_SMOTE2_pca, pca_model_SMOTE2 = apply_pca(X_SMOTE2, n_components)
year2_SMOTE_X_train, year2_SMOTE_X_test, year2_SMOTE_y_train, year2_SMOTE_y_test = train_test_split(X_SMOTE2_pca, y_SMOTE2, test_size=0.2, random_state=42)

# === Year 3 ===
# Random Oversampling
X_RO3 = year3_RO_scaled.drop('Class', axis=1)
y_RO3 = year3_RO_scaled['Class']
X_RO3_pca, pca_model_RO3 = apply_pca(X_RO3, n_components)
year3_RO_X_train, year3_RO_X_test, year3_RO_y_train, year3_RO_y_test = train_test_split(X_RO3_pca, y_RO3, test_size=0.2, random_state=42)

# Random Undersampling
X_RU3 = year3_RU_scaled.drop('Class', axis=1)
y_RU3 = year3_RU_scaled['Class']
X_RU3_pca, pca_model_RU3 = apply_pca(X_RU3, n_components)
year3_RU_X_train, year3_RU_X_test, year3_RU_y_train, year3_RU_y_test = train_test_split(X_RU3_pca, y_RU3, test_size=0.2, random_state=42)

# SMOTE Augmentation
X_SMOTE3 = year3_SMOTE_scaled.drop('Class', axis=1)
y_SMOTE3 = year3_SMOTE_scaled['Class']
X_SMOTE3_pca, pca_model_SMOTE3 = apply_pca(X_SMOTE3, n_components)
year3_SMOTE_X_train, year3_SMOTE_X_test, year3_SMOTE_y_train, year3_SMOTE_y_test = train_test_split(X_SMOTE3_pca, y_SMOTE3, test_size=0.2, random_state=42)

"""## **12.1 Random Oversampling**

### **12.1.1 XGBoost**
"""

# === Train on Year 1 and Year 2, Test on Year 3 ===
X_train = pd.concat([pd.DataFrame(year1_RO_X_train), pd.DataFrame(year2_RO_X_train)], axis=0)
y_train = pd.concat([pd.DataFrame(year1_RO_y_train), pd.DataFrame(year2_RO_y_train)], axis=0)

X_test = pd.DataFrame(year3_RO_X_test)
y_test = pd.DataFrame(year3_RO_y_test)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

import xgboost as xgb
#  XGBoost using best parameters
# 4.	Best parameters to use for further final training:
# o	Learning rate: 0.1 or 0.2
# o	Max_depth: 10 (frequent in high-performing models)
# o	N_estimators: 300 (most consistent value)



xgb_model = xgb.XGBClassifier(
    learning_rate=0.2,
    max_depth=10,
    n_estimators=300,
    random_state=42
)



xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

"""### **12.1.2 Bagging**"""

# Best Parameters for Bagging
# 	Base Estimator: Decision Tree with max_depth=None.
# 	Max Samples: 1.0 (Use all samples per tree).
# 	Max Features: 0.7 (Balanced performance with a subset of features).
# 	Number of Estimators (n_estimators): 100.

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=None),
    n_estimators=100,
    max_samples=1.0,
    max_features=0.7,
    random_state=42
)
bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

"""### **11.1.3 Random Forest**"""

# Best Parameters for Final Model:
# 	Max_depth: None (or 10 for regularization)
# 	Min_samples_leaf: 1 (or 2 for regularization)
# 	N_estimators: 100 to 200


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

random_forest_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_leaf=1,
    random_state=42
)
random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

"""### **11.1.4 SVM**"""

# Best Parameters for Final Model:
# 'C': 10
# 'gamma': 'scale', 'kernel': 'rbf'


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RO_X_train, year2_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year2_RO_y_train], axis=0)
X_test = year3_RO_X_test
y_test = year3_RO_y_test

SVM_model = SVC(
    C=10,
    gamma='scale',
    kernel='rbf',
    random_state=42
)
svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year2_RO_y_train, year3_RO_y_train], axis=0)
X_test = year1_RO_X_test
y_test = year1_RO_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RO_X_train, year3_RO_X_train], axis=0)
y_train = pd.concat([year1_RO_y_train, year3_RO_y_train], axis=0)
X_test = year2_RO_X_test
y_test = year2_RO_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

"""## **11.2 Random Undersampling**

### **11.2.1 XGBoost**
"""

import xgboost as xgb
#  XGBoost using best parameters
# 4.	Best parameters to use for further final training:
# o	Learning rate: 0.1 or 0.2
# o	Max_depth: 10 (frequent in high-performing models)
# o	N_estimators: 300 (most consistent value)

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

xgb_model = xgb.XGBClassifier(
    learning_rate=0.05,
    max_depth=10,
    n_estimators=300,
    random_state=42
)


xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

"""### **11.2.2 Bagging**"""

# Best Parameters for Bagging
# estimator__max_depth: None
# Max_samples: 0.5
# Max_features: 1.0
# N_estimators: 100


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=None),
    n_estimators=100,
    max_samples=0.5,
    max_features=1.0,
    random_state=42
)
bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

"""### **11.2.3 Random Forest**"""

# Best Parameters for Final Model:
# Max_depth: 20
# Min_samples_leaf: 1
# N_estimators: 100



# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

random_forest_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=20,
    min_samples_leaf=1,
    random_state=42
)
random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test


random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

"""### **11.2.4 SVM**"""

# Best Parameters for Final Model:
# 'C': 10
# 'gamma': 'scale', 'kernel': 'rbf'


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_RU_X_train, year2_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year2_RU_y_train], axis=0)
X_test = year3_RU_X_test
y_test = year3_RU_y_test

SVM_model = SVC(
    C=10,
    gamma='scale',
    kernel='rbf',
    random_state=42
)
svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year2_RU_y_train, year3_RU_y_train], axis=0)
X_test = year1_RU_X_test
y_test = year1_RU_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_RU_X_train, year3_RU_X_train], axis=0)
y_train = pd.concat([year1_RU_y_train, year3_RU_y_train], axis=0)
X_test = year2_RU_X_test
y_test = year2_RU_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

"""## **11.3 SMOTE**

### **11.3.1 XGBoost**
"""

import xgboost as xgb
#  XGBoost using best parameters
# 4.	Best parameters to use for further final training:
# o	Learning rate: 0.1 or 0.2
# o	Max_depth: 10 (frequent in high-performing models)
# o	N_estimators: 300 (most consistent value)

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

xgb_model = xgb.XGBClassifier(
    learning_rate=0.2,
    max_depth=10,
    n_estimators=300,
    random_state=42
)


xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

xgb_model_training(X_train, X_test, y_train, y_test, xgb_model)

"""### **11.3.2 Bagging**"""

# Best Parameters for Bagging
# estimator__max_depth: None
# estimator__max_depth: None
# Max_samples: 1.0
# Max_features: 0.7
# N_estimators: 100



# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=None),
    n_estimators=100,
    max_samples=1.0,
    max_features=0.7,
    random_state=42
)
bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

bagging_model_training(X_train, X_test, y_train, y_test, bagging_model)

"""### **11.3.3 Random Forest**"""

# Best Parameters for Final Model:
# Max_depth: None
# Min_samples_leaf: 1
# N_estimators: 100

# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

random_forest_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_leaf=1,
    random_state=42
)
random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test


random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

random_forest_model_training(X_train, X_test, y_train, y_test, random_forest_model)

"""### **11.3.4 SVM**"""

# Best Parameters for Final Model:
# 'C': 10
# 'gamma': 'scale', 'kernel': 'rbf'


# Train on Year 1 and Year 2, Test on Year 3
X_train = pd.concat([year1_SMOTE_X_train, year2_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year2_SMOTE_y_train], axis=0)
X_test = year3_SMOTE_X_test
y_test = year3_SMOTE_y_test

SVM_model = SVC(
    C=10,
    gamma='auto',
    kernel='rbf',
    random_state=42
)
svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 2 and Year 3, Test on Year 1
X_train = pd.concat([year2_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year2_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year1_SMOTE_X_test
y_test = year1_SMOTE_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

# Train on Year 1 and Year 3, Test on Year 2
X_train = pd.concat([year1_SMOTE_X_train, year3_SMOTE_X_train], axis=0)
y_train = pd.concat([year1_SMOTE_y_train, year3_SMOTE_y_train], axis=0)
X_test = year2_SMOTE_X_test
y_test = year2_SMOTE_y_test

svm_model_training(X_train, X_test, y_train, y_test, SVM_model)

"""# **13. Unsupervised Methods without PCA**

## **13.1 Year 1**

### **13.1.1 SMOTE**
"""

data1 = year1_SMOTE_scaled.copy()
features1 = data1.drop("Class", axis=1)
labels1 = data1["Class"]
print(labels1)
reduced_features1 = apply_pca(features1)
pcadf1 = reduced_features1.copy()
feature_names1 = features1.columns.tolist()
print(feature_names1)

"""#### **13.1.1.1 K Means Clustering**"""

find_optimal_k(features1)

"""The "elbow" point in this graph, which determines the optimal k, appears to be around 3. This is where the inertia (sum of squared distances) starts to level off, indicating diminishing returns for additional clusters.

But we know that our dataset has 2 classes, rice and cotton so we will go with k=2 clusters.
"""

kmeans_nopca_model1, kmeans_nopca_labels1 = kmeans_clustering(features1, 2)
print(kmeans_nopca_labels1)
pcadf1['kmeans_cluster'] = kmeans_nopca_labels1
print(kmeans_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=kmeans_nopca_labels1,
    title="K-means Clustering Without PCA",
    centroids=kmeans_nopca_model1.cluster_centers_
)

evaluate_clustering(kmeans_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, kmeans_nopca_labels1)

cluster_class_distribution(kmeans_nopca_labels1, labels1)

"""#### **13.1.1.2 Hirarchical Clustering**"""

plot_dendrogram(features1, max_depth=5)

"""##### **Ward Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="ward")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_ward'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Average Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="average")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_avg'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Centroid Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="centroid")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_centroid'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Complete Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="complete")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_complete'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""eres how each linkage method compares:

 1. **Ward Linkage:**
   - **Cluster Purity:** 0.7527
   - **Normalized Mutual Information (NMI):** 0.3310
   - **Silhouette Score:** 0.2533
   - **Class Distribution:**
     - Cluster 1: 6 datapoints with label 0, 1050 datapoints with label 1
     - Cluster 2: 2259 datapoints with label 0, 1081 datapoints with label 1
   - **Summary:** Ward linkage performs well in terms of cluster purity and NMI. Despite some misdistribution, it is a solid performer compared to the other linkages.

 2. **Average Linkage:**
   - **Cluster Purity:** 0.5152
   - **Normalized Mutual Information (NMI):** 0.0059
   - **Silhouette Score:** 0.2344
   - **Class Distribution:**
     - Cluster 1: 14 datapoints with label 0, 0 datapoints with label 1
     - Cluster 2: 2251 datapoints with label 0, 2131 datapoints with label 1
   - **Summary:** Average linkage gives poor performance. The cluster purity and NMI are very low, indicating poor clustering quality.

3. **Centroid Linkage:**
   - **Cluster Purity:** 0.5152
   - **Normalized Mutual Information (NMI):** 0.0004
   - **Silhouette Score:** 0.3718
   - **Class Distribution:**
     - Cluster 1: 2264 datapoints with label 0, 2131 datapoints with label 1
     - Cluster 2: 1 datapoint with label 0, 0 datapoints with label 1
   - **Summary:** Centroid linkage also shows poor performance, with a bad class distribution and a very low NMI, despite a slightly better silhouette score.

4. **Complete Linkage:**
   - **Cluster Purity:** 0.5152
   - **Normalized Mutual Information (NMI):** 0.0008
   - **Silhouette Score:** 0.1025
   - **Class Distribution:**
     - Cluster 1: 71 datapoints with label 0, 49 datapoints with label 1
     - Cluster 2: 2194 datapoints with label 0, 2082 datapoints with label 1
   - **Summary:** Complete linkage also provides poor results, with low cluster purity and NMI, and a very low silhouette score.

 **Comparison:**
- **Ward Linkage** outperforms all the other methods in terms of **cluster purity (0.7527)** and **NMI (0.3310)**, making it the best-performing linkage method here.
- The other linkages (Average, Centroid, and Complete) show much poorer results, particularly in **NMI** and **cluster purity**, which indicate less accurate clustering.

**Conclusion:**
- **Ward Linkage** is the best performer among the four methods. Despite some misdistribution, it gives the highest cluster purity, NMI, and reasonable silhouette scores.

#### **13.1.1.3 DBScan Clustering**

##### **Finding Optimal epsilon**

We usually take value of k>= dimension of data. Dimension of our data is 12. It is recommended to keep k between Dimentionality + 1 and 2 * Dimentionality.
"""

find_optimal_epsilon(features1, k=12)
find_optimal_epsilon(features1, k=24)

"""##### **Implementation**"""

# Define ranges for eps and min_samples
eps_range = [0.5, 1, 1.5, 2, 2.5, 3]
# eps_range = [2.0, 2.5, 3, 3.5, 4]
min_samples_range = [12, 15, 18, 21, 24]

# Assuming pca_df contains PCA components with columns ['PC1', 'PC2']
dbscan_experiment(features1, eps_range, min_samples_range, reduced_features1)

from sklearn.metrics import silhouette_score

def find_two_clusters(X, pca_df, eps_range, min_samples_range):
    for eps in eps_range:
        for min_samples in min_samples_range:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = dbscan.fit_predict(X)
            num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)

            if num_clusters == 2:
                noise_points = sum(cluster_labels == -1)
                print(f"eps={eps}, min_samples={min_samples}")
                print(f"Number of clusters (excluding noise): {num_clusters}")
                print(f"Number of noise points: {noise_points}")

                # Silhouette score evaluation
                if num_clusters > 1:
                    silhouette = silhouette_score(X, cluster_labels)
                    print(f"Silhouette score: {silhouette}")
                else:
                    print("Silhouette score: Not enough clusters for valid evaluation.")

                print("-" * 30)

# Define your eps and min_samples ranges
eps_range = [0.1 * i for i in range(5, 15)]  # Example: [0.5, 0.6, ..., 1.4]
min_samples_range = [10, 12, 15, 18, 20]

# Run the function
find_two_clusters(features1, reduced_features1, eps_range, min_samples_range)

dbscan_nopca_labels1 = dbscan_clustering(features1, eps=2, min_samples=12)
print(dbscan_nopca_labels1)
pcadf1['dbscan_cluster'] = dbscan_nopca_labels1
print(dbscan_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=dbscan_nopca_labels1,
    title="Dbscan Clustering Without PCA"
)

evaluate_clustering(dbscan_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, dbscan_nopca_labels1)

cluster_class_distribution(dbscan_nopca_labels1, labels1)

"""#### **13.1.1.4 Gausian Mixture Models**"""

gmm_nopca_labels1 = gmm_clustering(features1, n_clusters=2)
print(gmm_nopca_labels1)
pcadf1['gmm_cluster'] = gmm_nopca_labels1
print(gmm_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=gmm_nopca_labels1,
    title="GMM Clustering Without PCA"
)

evaluate_clustering(gmm_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, gmm_nopca_labels1)

cluster_class_distribution(gmm_nopca_labels1, labels1)

"""### **13.1.1.5 Model Comparison**

1. **K-Means Clustering**
- **Elbow Method**: Despite the elbow method suggesting \( k = 3 \), \( k \) was set to 2, aligning with the two classes in the labeled dataset.
- **Cluster Purity**: 0.7677 (highest among all models).
- **Normalized Mutual Information (NMI)**: 0.2699.
- **Silhouette Score**: 0.2505.
- **Confusion Matrix**: Showed good distribution across the two clusters.
- **Class Distribution**:
  - Cluster 0: 2133 datapoints with label 0, 889 datapoints with label 1.
  - Cluster 1: 132 datapoints with label 0, 1242 datapoints with label 1.
  
**Summary**:  
K-Means performed reasonably well, with the highest cluster purity and decent silhouette score, reflecting well-separated clusters. However, the NMI suggests moderate agreement between true labels and clustering.

---

2. **Hierarchical Clustering**

a. **Ward Linkage**
- **Cluster Purity**: 0.7527.
- **NMI**: 0.3310 (highest among all hierarchical methods).
- **Silhouette Score**: 0.2533.
- **Class Distribution**:
  - Cluster 1: 6 datapoints with label 0, 1050 datapoints with label 1.
  - Cluster 2: 2259 datapoints with label 0, 1081 datapoints with label 1.
  
**Summary**:  
Ward linkage performed best among hierarchical clustering methods, achieving high cluster purity and the best NMI. However, some misclassifications indicate overlapping clusters.

b. **Average Linkage**
- **Cluster Purity**: 0.5152.
- **NMI**: 0.0059 (very low).
- **Silhouette Score**: 0.2344.
- **Class Distribution**:
  - Cluster 1: 14 datapoints with label 0, 0 datapoints with label 1.
  - Cluster 2: 2251 datapoints with label 0, 2131 datapoints with label 1.
  
**Summary**:  
Average linkage showed poor performance, with very low NMI and a skewed class distribution.

c. **Centroid Linkage**
- **Cluster Purity**: 0.5152.
- **NMI**: 0.0004.
- **Silhouette Score**: 0.3718 (highest among hierarchical methods).
- **Class Distribution**:
  - Cluster 1: 2264 datapoints with label 0, 2131 datapoints with label 1.
  - Cluster 2: 1 datapoint with label 0, 0 datapoints with label 1.
  
**Summary**:  
Centroid linkage provided a slightly better silhouette score but failed in NMI and cluster purity, indicating poor clustering quality.

d. **Complete Linkage**
- **Cluster Purity**: 0.5152.
- **NMI**: 0.0008.
- **Silhouette Score**: 0.1025.
- **Class Distribution**:
  - Cluster 1: 71 datapoints with label 0, 49 datapoints with label 1.
  - Cluster 2: 2194 datapoints with label 0, 2082 datapoints with label 1.
  
**Summary**:  
Complete linkage performed the worst, with extremely low scores and poor clustering.

---

3. **DBScan Clustering**
- **Cluster Purity**: 0.5152.
- **NMI**: 0.000005 (almost negligible).
- **Silhouette Score**: 0.1555.
- **Class Distribution**:
  - Cluster -1: 35 datapoints with label 0, 34 datapoints with label 1 (noise).
  - Cluster 0: 2230 datapoints with label 0, 2097 datapoints with label 1.
  
**Summary**:  
DBScan struggled to form meaningful clusters due to overlapping data. It either formed one large cluster or multiple fragmented clusters, making it unsuitable for this dataset.

---

4. **Gaussian Mixture Model (GMM) Clustering**
- **Cluster Purity**: 0.7634.
- **NMI**: 0.3368 (highest overall).
- **Silhouette Score**: 0.2542.
- **Class Distribution**:
  - Cluster 0: 2251 datapoints with label 0, 1026 datapoints with label 1.
  - Cluster 1: 14 datapoints with label 0, 1105 datapoints with label 1.
  
**Summary**:  
GMM performed well, achieving high NMI and cluster purity. Its probabilistic nature helped it handle overlapping clusters better than K-Means or hierarchical methods.

---

**Comparison Table**

| Model              | Cluster Purity | NMI      | Silhouette Score | Notes                                     |
|---------------------|----------------|----------|------------------|-------------------------------------------|
| K-Means            | **0.7677**     | 0.2699   | 0.2505           | Best cluster purity; struggles with NMI.  |
| Ward Linkage       | 0.7527         | 0.3310   | 0.2533           | Best hierarchical method.                 |
| Average Linkage    | 0.5152         | 0.0059   | 0.2344           | Poor performance; skewed clusters.        |
| Centroid Linkage   | 0.5152         | 0.0004   | 0.3718           | High silhouette but poor clustering.      |
| Complete Linkage   | 0.5152         | 0.0008   | 0.1025           | Worst hierarchical method.                |
| DBScan             | 0.5152         | 0.000005 | 0.1555           | Unsuitable for overlapping data.          |
| GMM                | 0.7634         | **0.3368** | **0.2542**      | Best NMI; strong performance overall.     |

---

 **Conclusion**
1. **Best Model**: Gaussian Mixture Model (GMM) due to its ability to handle overlapping clusters and achieving the highest NMI and strong cluster purity.
2. **Runner-Up**: K-Means Clustering, with the highest cluster purity but slightly lower NMI.
3. **Hierarchical Clustering**: Ward linkage is the best option, but overall, hierarchical clustering methods are less suitable for this dataset.
4. **Unsuitable Models**: DBScan and other linkage methods in hierarchical clustering fail to handle the overlapping clusters effectively.

## **13.2 Year 2**

### **13.2.1 SMOTE**
"""

data1 = year2_SMOTE_scaled.copy()
features1 = data1.drop("Class", axis=1)
labels1 = data1["Class"]
print(labels1)
reduced_features1 = apply_pca(features1)
pcadf1 = reduced_features1.copy()
feature_names1 = features1.columns.tolist()
print(feature_names1)

"""#### **13.1.1.1 K Means Clustering**"""

find_optimal_k(features1)

"""The "elbow" point in this graph, which determines the optimal k, appears to be around 3. This is where the inertia (sum of squared distances) starts to level off, indicating diminishing returns for additional clusters.

But we know that our dataset has 2 classes, rice and cotton so we will go with k=2 clusters.
"""

kmeans_nopca_model1, kmeans_nopca_labels1 = kmeans_clustering(features1, 2)
print(kmeans_nopca_labels1)
pcadf1['kmeans_cluster'] = kmeans_nopca_labels1
print(kmeans_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=kmeans_nopca_labels1,
    title="K-means Clustering Without PCA",
    centroids=kmeans_nopca_model1.cluster_centers_
)

evaluate_clustering(kmeans_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, kmeans_nopca_labels1)

cluster_class_distribution(kmeans_nopca_labels1, labels1)

"""#### **13.1.1.2 Hirarchical Clustering**"""

plot_dendrogram(features1, max_depth=5)

"""##### **Ward Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="ward")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_ward'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Average Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="average")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_avg'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Centroid Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="centroid")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_centroid'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Complete Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="complete")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_complete'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""#### **13.1.1.3 DBScan Clustering**

##### **Finding Optimal epsilon**

We usually take value of k>= dimension of data. Dimension of our data is 12. It is recommended to keep k between Dimentionality + 1 and 2 * Dimentionality.
"""

find_optimal_epsilon(features1, k=12)
find_optimal_epsilon(features1, k=24)

"""##### **Implementation**"""

# Define ranges for eps and min_samples
eps_range = [0.5, 1, 1.5, 2, 2.5, 3]
# eps_range = [2.0, 2.5, 3, 3.5, 4]
min_samples_range = [12, 15, 18, 21, 24]

# Assuming pca_df contains PCA components with columns ['PC1', 'PC2']
dbscan_experiment(features1, eps_range, min_samples_range, reduced_features1)

dbscan_nopca_labels1 = dbscan_clustering(features1, eps=1.5, min_samples=21)
print(dbscan_nopca_labels1)
pcadf1['dbscan_cluster'] = dbscan_nopca_labels1
print(dbscan_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=dbscan_nopca_labels1,
    title="Dbscan Clustering Without PCA"
)

evaluate_clustering(dbscan_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, dbscan_nopca_labels1)

cluster_class_distribution(dbscan_nopca_labels1, labels1)

"""#### **13.1.1.4 Gausian Mixture Models**"""

gmm_nopca_labels1 = gmm_clustering(features1, n_clusters=2)
print(gmm_nopca_labels1)
pcadf1['gmm_cluster'] = gmm_nopca_labels1
print(gmm_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=gmm_nopca_labels1,
    title="GMM Clustering Without PCA"
)

evaluate_clustering(gmm_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, gmm_nopca_labels1)

cluster_class_distribution(gmm_nopca_labels1, labels1)

"""### **Model Comparison**

### **1. K-Means Clustering**

#### **Evaluation Metrics**
- **Cluster Purity:** 0.801
- **NMI:** 0.3491
- **Silhouette Score:** 0.2243

#### **Performance**
- Visualization reveals clear separations when \( k=2 \) (forced due to labels).
- **Class Distribution:**
  - Cluster 0: 11563 (label 0), 4148 (label 1)
  - Cluster 1: 372 (label 0), 6631 (label 1)

#### **Strengths**
- Best purity score among all methods.
- Suitable for datasets with distinct spherical clusters.

#### **Limitations**
- Lower silhouette score suggests suboptimal cluster compactness.

---

### **2. Hierarchical Clustering**

#### **Evaluation Metrics**
##### Ward Linkage:
- **Cluster Purity:** 0.758
- **NMI:** 0.3101
- **Silhouette Score:** 0.2043

##### Average Linkage:
- **Cluster Purity:** 0.525
- **NMI:** ~0
- **Silhouette Score:** 0.1832

##### Centroid Linkage:
- **Cluster Purity:** 0.525
- **NMI:** 0.0163
- **Silhouette Score:** 0.2981

##### Complete Linkage:
- **Cluster Purity:** 0.525
- **NMI:** 0.0391
- **Silhouette Score:** 0.2333

#### **Performance**
- Best configuration: **Ward Linkage**.
- **Class Distribution (Ward Linkage):**
  - Cluster 1: 146 (label 0), 5429 (label 1)
  - Cluster 2: 11789 (label 0), 5350 (label 1)

#### **Strengths**
- Can capture different shapes and densities of clusters.
- Good performance with Ward linkage.

#### **Limitations**
- Poor performance with other linkage methods.
- Lower purity compared to K-Means.

---

### **3. DBSCAN Clustering**

#### **Evaluation Metrics**
- **Cluster Purity:** 0.5267
- **NMI:** 0.0019
- **Silhouette Score:** 0.0161

#### **Performance**
- Best configuration: \( \epsilon=1.5, \text{min_samples}=18 \).
- **Class Distribution:**
  - Cluster -1 (Noise): 1085 (label 0), 1004 (label 1)
  - Cluster 0: 10850 (label 0), 9746 (label 1)
  - Cluster 1: 0 (label 0), 29 (label 1)

#### **Strengths**
- Can detect noise and anomalies.

#### **Limitations**
- Overlapping clusters and noise points lead to suboptimal results.
- Negative silhouette score indicates poor clustering quality.

---

### **4. Gaussian Mixture Model (GMM)**

#### **Evaluation Metrics**
- **Cluster Purity:** 0.776
- **NMI:** 0.3586
- **Silhouette Score:** 0.2129

#### **Performance**
- **Class Distribution:**
  - Cluster 0: 11887 (label 0), 5041 (label 1)
  - Cluster 1: 48 (label 0), 5738 (label 1)

#### **Strengths**
- Handles overlapping clusters better than DBSCAN.
- Comparable NMI to K-Means.

#### **Limitations**
- Slightly lower purity than K-Means.
- Assumes clusters follow a Gaussian distribution, which may not align with the data.

---

### **Summary of Model Comparison**

| **Model**              | **Cluster Purity** | **NMI**  | **Silhouette Score** | **Best Use Case**                                   |
|-------------------------|--------------------|----------|-----------------------|----------------------------------------------------|
| **K-Means**            | **0.801**          | 0.3491   | 0.2243                | Well-separated clusters with equal densities.     |
| **Hierarchical (Ward)** | 0.758              | 0.3101   | 0.2043                | Non-spherical clusters with varying densities.    |
| **DBSCAN**             | 0.5267             | 0.0019   | 0.0161                | Noise detection and arbitrary-shaped clusters.    |
| **GMM**                | 0.776              | **0.3586** | 0.2129                | Overlapping clusters with Gaussian distributions. |

---

### **Recommendations**
1. **K-Means:** Best overall performance for purity and NMI; a good fit for your labeled data with two classes.
2. **Hierarchical Clustering (Ward):** Suitable as an alternative if more interpretability is needed.
3. **DBSCAN:** Avoid unless the goal is to identify noise points.
4. **GMM:** Useful if cluster overlaps are expected and Gaussian assumptions hold.

## **13.3 Year 3**

### **13.3.1 SMOTE**
"""

data1 = year3_SMOTE_scaled.copy()
features1 = data1.drop("Class", axis=1)
labels1 = data1["Class"]
print(labels1)
reduced_features1 = apply_pca(features1)
pcadf1 = reduced_features1.copy()
feature_names1 = features1.columns.tolist()
print(feature_names1)

"""#### **13.3.1.1 K Means Clustering**"""

find_optimal_k(features1)

"""The "elbow" point in this graph, which determines the optimal k, appears to be around 3. This is where the inertia (sum of squared distances) starts to level off, indicating diminishing returns for additional clusters.

But we know that our dataset has 2 classes, rice and cotton so we will go with k=2 clusters.
"""

kmeans_nopca_model1, kmeans_nopca_labels1 = kmeans_clustering(features1, 2)
print(kmeans_nopca_labels1)
pcadf1['kmeans_cluster'] = kmeans_nopca_labels1
print(kmeans_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=kmeans_nopca_labels1,
    title="K-means Clustering Without PCA",
    centroids=kmeans_nopca_model1.cluster_centers_
)

evaluate_clustering(kmeans_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, kmeans_nopca_labels1)

cluster_class_distribution(kmeans_nopca_labels1, labels1)

"""#### **13.1.1.2 Hirarchical Clustering**"""

plot_dendrogram(features1, max_depth=5)

"""##### **Ward Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="ward")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_ward'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Average Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="average")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_avg'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Centroid Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="centroid")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_centroid'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Complete Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="complete")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_complete'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""#### **13.1.1.3 DBScan Clustering**

##### **Finding Optimal epsilon**

We usually take value of k>= dimension of data. Dimension of our data is 12. It is recommended to keep k between Dimentionality + 1 and 2 * Dimentionality.
"""

find_optimal_epsilon(features1, k=12)
find_optimal_epsilon(features1, k=24)

"""##### **Implementation**"""

# Define ranges for eps and min_samples
eps_range = [1.55, 1.6]
# eps_range = [2.0, 2.5, 3, 3.5, 4]
min_samples_range = [12, 15, 18, 21, 24]

dbscan_experiment(features1, eps_range, min_samples_range, reduced_features1)

dbscan_nopca_labels1 = dbscan_clustering(features1, eps=1.7, min_samples=12)
print(dbscan_nopca_labels1)
pcadf1['dbscan_cluster'] = dbscan_nopca_labels1
print(dbscan_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=dbscan_nopca_labels1,
    title="Dbscan Clustering Without PCA"
)

evaluate_clustering(dbscan_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, dbscan_nopca_labels1)

cluster_class_distribution(dbscan_nopca_labels1, labels1)

"""#### **13.1.1.4 Gausian Mixture Models**"""

gmm_nopca_labels1 = gmm_clustering(features1, n_clusters=2)
print(gmm_nopca_labels1)
pcadf1['gmm_cluster'] = gmm_nopca_labels1
print(gmm_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=gmm_nopca_labels1,
    title="GMM Clustering Without PCA"
)

evaluate_clustering(gmm_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, gmm_nopca_labels1)

cluster_class_distribution(gmm_nopca_labels1, labels1)

"""**Model Comparison**

SMOTE was used to address class imbalance in the dataset, ensuring more robust clustering by generating synthetic samples for the minority class. After applying SMOTE, various clustering methods were evaluated.

---

### **13.3.1.1 K-Means Clustering**

#### **Elbow Method**  
- **Optimal k:** Although the elbow method suggested \(k = 3\), we set \(k = 2\) as the dataset is labeled for binary classification.

#### **Visualization**  
Clustering results were visualized, showing distinct clusters for the majority and minority classes.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.8014


- **Normalized Mutual Information (NMI):** 0.3535
- **Silhouette Score:** 0.2928

#### **Class Distribution Among Clusters**
- Cluster 0: Predominantly label 0 (11,093) with fewer label 1 samples (4,114).  
- Cluster 1: Predominantly label 1 (7,050) with fewer label 0 samples (382).

---

### **13.3.1.2 Hierarchical Clustering**

#### **Dendrogram**  
- Dendrogram suggested the dataset has moderate cluster separation.

#### **Ward Linkage Visualization**  
Visualization confirmed the existence of two primary clusters.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.8098


- **Normalized Mutual Information (NMI):** 0.3852  
- **Silhouette Score:** 0.2802

#### **Class Distribution Among Clusters**
- Cluster 1: Predominantly label 1 (1,050).  
- Cluster 2: Predominantly label 0 (2,259).

---

### **13.3.1.3 DBSCAN Clustering**

#### **Elbow Method**
- Optimal parameters: \( \text{eps} = 1.5 \), \( \text{min\_samples} = 12 \).
- Experimentation across multiple parameter combinations did not yield satisfactory results due to overlapping clusters.

#### **Observations**
- DBSCAN struggled with overlapping clusters, making either many clusters or one large cluster with noise.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.5069


- **Normalized Mutual Information (NMI):** 0.0142  
- **Silhouette Score:** 0.0897  

#### **Class Distribution Among Clusters**
- Cluster -1: Mostly noise.  
- Cluster 0: Mixed labels, showing poor separation.

---

### **13.3.1.4 Gaussian Mixture Model (GMM) Clustering**

#### **Visualization**
- GMM clustering displayed a smoother cluster separation compared to K-Means, but still struggled with perfect segmentation.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.7872
- **Confusion Matrix:**

  | **Cluster** | **Label 0** | **Label 1** |
  |-------------|-------------|-------------|
  | **0**       | 11,401      | 4,744       |
  | **1**       | 74          | 6,420       |

- **Normalized Mutual Information (NMI):** 0.3766  
- **Silhouette Score:** 0.2799

#### **Class Distribution Among Clusters**
- Cluster 0: Predominantly label 0.  
- Cluster 1: Predominantly label 1.

---

### **Summary of Model Performance**

| **Metric/Model**      | **K-Means** | **Hierarchical** | **DBSCAN** | **GMM**  |
|------------------------|-------------|------------------|------------|----------|
| **Cluster Purity**     | 0.8014      | 0.8098           | 0.5069     | 0.7872   |
| **NMI**               | 0.3535      | 0.3852           | 0.0142     | 0.3766   |
| **Silhouette Score**   | 0.2928      | 0.2802           | 0.0897     | 0.2799   |

### **Conclusion**
- **Best Performer:** **Hierarchical Clustering** achieved the highest cluster purity and NMI.  
- **Worst Performer:** **DBSCAN** failed due to overlapping clusters.  
- **GMM and K-Means:** Performed similarly, with GMM slightly better in handling data complexity.  
- **Recommendation:** Use Hierarchical Clustering for this dataset as it balances performance and interpretability effectively.

## **13.4 All Years Combined**

### **13.4.1 SMOTE**
"""

combined_df = pd.concat([year1_SMOTE_scaled, year2_SMOTE_scaled, year3_SMOTE_scaled], axis=0, ignore_index=True)
data1 = combined_df.copy()
features1 = data1.drop("Class", axis=1)
labels1 = data1["Class"]
print(labels1)
reduced_features1 = apply_pca(features1)
pcadf1 = reduced_features1.copy()
feature_names1 = features1.columns.tolist()
print(feature_names1)

"""#### **13.3.1.1 K Means Clustering**"""

find_optimal_k(features1)

"""The "elbow" point in this graph, which determines the optimal k, appears to be around 3. This is where the inertia (sum of squared distances) starts to level off, indicating diminishing returns for additional clusters.

But we know that our dataset has 2 classes, rice and cotton so we will go with k=2 clusters.
"""

kmeans_nopca_model1, kmeans_nopca_labels1 = kmeans_clustering(features1, 2)
print(kmeans_nopca_labels1)
pcadf1['kmeans_cluster'] = kmeans_nopca_labels1
print(kmeans_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=kmeans_nopca_labels1,
    title="K-means Clustering Without PCA",
    centroids=kmeans_nopca_model1.cluster_centers_
)

evaluate_clustering(kmeans_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, kmeans_nopca_labels1)

cluster_class_distribution(kmeans_nopca_labels1, labels1)

"""#### **13.1.1.2 Hirarchical Clustering**"""

plot_dendrogram(features1, max_depth=5)

"""##### **Ward Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="ward")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_ward'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Average Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="average")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_avg'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Centroid Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="centroid")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_centroid'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Complete Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(features1, n_clusters=2, method="complete")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_complete'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering Without PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""#### **13.1.1.3 DBScan Clustering**

##### **Finding Optimal epsilon**

We usually take value of k>= dimension of data. Dimension of our data is 12. It is recommended to keep k between Dimentionality + 1 and 2 * Dimentionality.
"""

find_optimal_epsilon(features1, k=12)
find_optimal_epsilon(features1, k=24)

"""we can see form graph that eps is 1.5"""

# Define ranges for eps and min_samples
eps_range = [1.55]
# eps_range = [2.0, 2.5, 3, 3.5, 4]
min_samples_range = [12, 20, 24]

# Assuming pca_df contains PCA components with columns ['PC1', 'PC2']
dbscan_experiment(features1, eps_range, min_samples_range, reduced_features1)

"""##### **Implementation**"""

dbscan_nopca_labels1 = dbscan_clustering(features1, eps=1.55, min_samples=12)
print(dbscan_nopca_labels1)
pcadf1['dbscan_cluster'] = dbscan_nopca_labels1
print(dbscan_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=dbscan_nopca_labels1,
    title="Dbscan Clustering Without PCA"
)

evaluate_clustering(dbscan_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, dbscan_nopca_labels1)

cluster_class_distribution(dbscan_nopca_labels1, labels1)

"""#### **13.1.1.4 Gausian Mixture Models**"""

gmm_nopca_labels1 = gmm_clustering(features1, n_clusters=2)
print(gmm_nopca_labels1)
pcadf1['gmm_cluster'] = gmm_nopca_labels1
print(gmm_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=gmm_nopca_labels1,
    title="GMM Clustering Without PCA"
)

evaluate_clustering(gmm_nopca_labels1, labels1, features1)

plot_comparison(reduced_features1, labels1, gmm_nopca_labels1)

cluster_class_distribution(gmm_nopca_labels1, labels1)

"""**Model Comparison**

SMOTE was used to address class imbalance in the dataset, ensuring more robust clustering by generating synthetic samples for the minority class. After applying SMOTE, various clustering methods were evaluated.

---

### **13.3.1.1 K-Means Clustering**

#### **Elbow Method**  
- **Optimal k:** Although the elbow method suggested \(k = 3\), we set \(k = 2\) as the dataset is labeled for binary classification.

#### **Visualization**  
Clustering results were visualized, showing distinct clusters for the majority and minority classes.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.8014


- **Normalized Mutual Information (NMI):** 0.3535
- **Silhouette Score:** 0.2928

#### **Class Distribution Among Clusters**
- Cluster 0: Predominantly label 0 (11,093) with fewer label 1 samples (4,114).  
- Cluster 1: Predominantly label 1 (7,050) with fewer label 0 samples (382).

---

### **13.3.1.2 Hierarchical Clustering**

#### **Dendrogram**  
- Dendrogram suggested the dataset has moderate cluster separation.

#### **Ward Linkage Visualization**  
Visualization confirmed the existence of two primary clusters.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.8098


- **Normalized Mutual Information (NMI):** 0.3852  
- **Silhouette Score:** 0.2802

#### **Class Distribution Among Clusters**
- Cluster 1: Predominantly label 1 (1,050).  
- Cluster 2: Predominantly label 0 (2,259).

---

### **13.3.1.3 DBSCAN Clustering**

#### **Elbow Method**
- Optimal parameters: \( \text{eps} = 1.5 \), \( \text{min\_samples} = 12 \).
- Experimentation across multiple parameter combinations did not yield satisfactory results due to overlapping clusters.

#### **Observations**
- DBSCAN struggled with overlapping clusters, making either many clusters or one large cluster with noise.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.5069


- **Normalized Mutual Information (NMI):** 0.0142  
- **Silhouette Score:** 0.0897  

#### **Class Distribution Among Clusters**
- Cluster -1: Mostly noise.  
- Cluster 0: Mixed labels, showing poor separation.

---

### **13.3.1.4 Gaussian Mixture Model (GMM) Clustering**

#### **Visualization**
- GMM clustering displayed a smoother cluster separation compared to K-Means, but still struggled with perfect segmentation.

#### **Evaluation Metrics**
- **Cluster Purity:** 0.7872
- **Confusion Matrix:**

  | **Cluster** | **Label 0** | **Label 1** |
  |-------------|-------------|-------------|
  | **0**       | 11,401      | 4,744       |
  | **1**       | 74          | 6,420       |

- **Normalized Mutual Information (NMI):** 0.3766  
- **Silhouette Score:** 0.2799

#### **Class Distribution Among Clusters**
- Cluster 0: Predominantly label 0.  
- Cluster 1: Predominantly label 1.

---

### **Summary of Model Performance**

| **Metric/Model**      | **K-Means** | **Hierarchical** | **DBSCAN** | **GMM**  |
|------------------------|-------------|------------------|------------|----------|
| **Cluster Purity**     | 0.8014      | 0.8098           | 0.5069     | 0.7872   |
| **NMI**               | 0.3535      | 0.3852           | 0.0142     | 0.3766   |
| **Silhouette Score**   | 0.2928      | 0.2802           | 0.0897     | 0.2799   |

### **Conclusion**
- **Best Performer:** **Hierarchical Clustering** achieved the highest cluster purity and NMI.  
- **Worst Performer:** **DBSCAN** failed due to overlapping clusters.  
- **GMM and K-Means:** Performed similarly, with GMM slightly better in handling data complexity.  
- **Recommendation:** Use Hierarchical Clustering for this dataset as it balances performance and interpretability effectively.

# **14. All Years Combined with pca**

### **14.1.1 SMOTE**
"""

combined_df = pd.concat([year1_SMOTE_scaled, year2_SMOTE_scaled, year3_SMOTE_scaled], axis=0, ignore_index=True)
data1 = combined_df.copy()
features1 = data1.drop("Class", axis=1)
labels1 = data1["Class"]
print(labels1)
reduced_features1 = apply_pca(features1)
pcadf1 = reduced_features1.copy()
feature_names1 = features1.columns.tolist()
print(feature_names1)

"""#### **14.1.1.1 K Means Clustering**"""

find_optimal_k(reduced_features1)

"""The "elbow" point in this graph, which determines the optimal k, appears to be around 3. This is where the inertia (sum of squared distances) starts to level off, indicating diminishing returns for additional clusters.

But we know that our dataset has 2 classes, rice and cotton so we will go with k=2 clusters.
"""

kmeans_nopca_model1, kmeans_nopca_labels1 = kmeans_clustering(reduced_features1, 2)
print(kmeans_nopca_labels1)
pcadf1['kmeans_cluster'] = kmeans_nopca_labels1
print(kmeans_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=kmeans_nopca_labels1,
    title="K-means Clustering With PCA",
    centroids=kmeans_nopca_model1.cluster_centers_
)

evaluate_clustering(kmeans_nopca_labels1, labels1, reduced_features1)

plot_comparison(reduced_features1, labels1, kmeans_nopca_labels1)

cluster_class_distribution(kmeans_nopca_labels1, labels1)

"""#### **13.1.1.2 Hirarchical Clustering**"""

plot_dendrogram(reduced_features1, max_depth=5)

"""##### **Ward Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(reduced_features1, n_clusters=2, method="ward")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_ward'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering With PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, reduced_features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Average Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(reduced_features1, n_clusters=2, method="average")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_avg'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering With PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, reduced_features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Centroid Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(reduced_features1, n_clusters=2, method="centroid")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_centroid'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering With PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, reduced_features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""##### **Complete Linkage**"""

hirarchical_nopca_labels1 = hierarchical_clustering(reduced_features1, n_clusters=2, method="complete")
print(hirarchical_nopca_labels1)
pcadf1['hirarchical_cluster_complete'] = hirarchical_nopca_labels1
print(hirarchical_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=hirarchical_nopca_labels1,
    title="Hierarchical Clustering With PCA"
)

evaluate_clustering(hirarchical_nopca_labels1, labels1, reduced_features1)

plot_comparison(reduced_features1, labels1, hirarchical_nopca_labels1)

cluster_class_distribution(hirarchical_nopca_labels1, labels1)

"""#### **13.1.1.3 DBScan Clustering**

##### **Finding Optimal epsilon**

We usually take value of k>= dimension of data. Dimension of our data is 12. It is recommended to keep k between Dimentionality + 1 and 2 * Dimentionality.
"""

find_optimal_epsilon(reduced_features1, k=4)

"""we can see form graph that eps is 0.1. Still we check for different values."""

# Define ranges for eps and min_samples
eps_range = [0.4]
# eps_range = [2.0, 2.5, 3, 3.5, 4]
min_samples_range = [4]

# Assuming pca_df contains PCA components with columns ['PC1', 'PC2']
dbscan_experiment(reduced_features1, eps_range, min_samples_range, reduced_features1)

"""##### **Implementation**"""

dbscan_nopca_labels1 = dbscan_clustering(reduced_features1, eps=0.4, min_samples=4)
print(dbscan_nopca_labels1)
pcadf1['dbscan_cluster'] = dbscan_nopca_labels1
print(dbscan_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=dbscan_nopca_labels1,
    title="Dbscan Clustering With PCA"
)

evaluate_clustering(dbscan_nopca_labels1, labels1, reduced_features1)

plot_comparison(reduced_features1, labels1, dbscan_nopca_labels1)

cluster_class_distribution(dbscan_nopca_labels1, labels1)

"""#### **13.1.1.4 Gausian Mixture Models**"""

gmm_nopca_labels1 = gmm_clustering(reduced_features1, n_clusters=2)
print(gmm_nopca_labels1)
pcadf1['gmm_cluster'] = gmm_nopca_labels1
print(gmm_nopca_labels1.shape)
print(labels1.shape)

pcadf1

# Example usage:
visualize_clusters(
    data=reduced_features1,
    labels=gmm_nopca_labels1,
    title="GMM Clustering With PCA"
)

evaluate_clustering(gmm_nopca_labels1, labels1, reduced_features1)

plot_comparison(reduced_features1, labels1, gmm_nopca_labels1)

cluster_class_distribution(gmm_nopca_labels1, labels1)

"""### Detailed Model Comparison Based on Dataset

#### **Clustering Models: Performance Analysis**

This comparison evaluates the clustering models applied to the dataset processed with PCA and SMOTE. Various methods were tested, and their performance metrics, including Cluster Purity, Normalized Mutual Information (NMI), Silhouette Score, and Class Distribution, were analyzed in detail.

---

### **1. K-Means Clustering**
- **Elbow Method**:  
  - Suggested \( k=3 \), but due to the binary nature of the dataset, \( k=2 \) was chosen.
  
- **Evaluation Metrics**:  
  - **Cluster Purity**: 0.6757  
  - **NMI**: 0.0930  
  - **Silhouette Score**: 0.3694  

- **Class Distribution**:  
  - **Cluster 0**: 6002 (label 0), 14021 (label 1)  
  - **Cluster 1**: 19673 (label 0), 10053 (label 1)  

- **Observation**:  
  K-Means demonstrated moderate clustering quality, with a fair silhouette score suggesting the clusters are distinguishable to some extent.

---

### **2. Hierarchical Clustering**

#### **a. Ward Linkage**
- **Evaluation Metrics**:  
  - **Cluster Purity**: 0.6062  
  - **NMI**: 0.0360  
  - **Silhouette Score**: 0.3241  

- **Class Distribution**:  
  - **Cluster 1**: 13390 (label 0), 7307 (label 1)  
  - **Cluster 2**: 12285 (label 0), 16767 (label 1)  

- **Observation**:  
  Ward linkage showed poor performance in terms of NMI, but the silhouette score indicates moderate cluster separation.

#### **b. Average Linkage**
- **Evaluation Metrics**:  
  - **Cluster Purity**: 0.6757  
  - **NMI**: 0.1350  
  - **Silhouette Score**: 0.3417  

- **Class Distribution**:  
  - **Cluster 1**: 1848 (label 0), 9788 (label 1)  
  - **Cluster 2**: 23827 (label 0), 14286 (label 1)  

- **Observation**:  
  Slight improvement in NMI and silhouette score compared to Ward linkage. It demonstrates better separation of clusters.

#### **c. Complete Linkage**
- **Evaluation Metrics**:  
  - **Cluster Purity**: 0.6942  
  - **NMI**: 0.1184  
  - **Silhouette Score**: 0.3643  

- **Class Distribution**:  
  - **Cluster 1**: 20920 (label 0), 10460 (label 1)  
  - **Cluster 2**: 4755 (label 0), 13614 (label 1)  

- **Observation**:  
  The best among hierarchical methods, with a balance of cluster purity and silhouette score.

---

### **3. DBScan Clustering**
- **Elbow Method**:  
  - Suggested \( \epsilon = 0.1 \) with a minimum sample size of 4. Various configurations of \( \epsilon \) and minpts were tested.

- **Evaluation Metrics**:  
  - **Cluster Purity**: 0.5161  
  - **NMI**: 0.0037  
  - **Silhouette Score**: -0.0534  

- **Class Distribution**:  
  - **Cluster -1**: 2 (label 0), 0 (label 1)  
  - **Cluster 0**: 25673 (label 0), 24074 (label 1)  

- **Observation**:  
  DBScan performed poorly with a negative silhouette score, indicating overlapping or poorly separated clusters.

---

### **Summary Table**

| **Clustering Method** | **Cluster Purity** | **NMI**   | **Silhouette Score** | **Best Cluster (Label 1)** | **Best Cluster (Label 0)** |
|------------------------|--------------------|-----------|-----------------------|----------------------------|----------------------------|
| **K-Means**            | 0.6757            | 0.0930    | 0.3694                | 14021                      | 19673                      |
| **Ward Linkage**       | 0.6062            | 0.0360    | 0.3241                | 16767                      | 13390                      |
| **Average Linkage**    | 0.6757            | 0.1350    | 0.3417                | 14286                      | 23827                      |
| **Complete Linkage**   | 0.6942            | 0.1184    | 0.3643                | 13614                      | 20920                      |
| **DBScan**             | 0.5161            | 0.0037    | -0.0534               | 24074                      | 25673                      |

---

### **Key Insights**
1. **Best Model**:  
   - Complete Linkage Hierarchical Clustering achieved the highest cluster purity and a decent silhouette score.
   
2. **Worst Model**:  
   - DBScan struggled to identify meaningful clusters due to its reliance on density parameters.

3. **General Performance**:  
   - K-Means and Hierarchical methods (Average and Complete Linkage) outperformed DBScan, particularly in terms of silhouette score and NMI.

"""

